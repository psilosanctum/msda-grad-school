---
title: Bookbinders Case Study 2
date: 2024-10-06
date-format: short
execute: 
  warning: false
  error: false
---

```{r}
# Load necessary libraries
library(readxl)    
library(ROSE)      
library(caret)
library(tidyverse)
library(caretEnsemble)
library(gridExtra)
library(corrplot)
library(e1071)
library(pROC)
library(car)
library(knitr)
library(ggplot2)
```
```{r}
# Step 1: Load the data from the Excel file
train_data <- read_excel("BBBC-Train.xlsx")  # Replace with the correct path
test_data <- read_excel("BBBC-Test.xlsx")  # Replace with the correct path

#ifelse(train_data$Gender == 0, 'Female', 'Male')
train_data$Gender <- as.factor(train_data$Gender)
levels(train_data$Gender) <- c('Female', 'Male')

#ifelse(train_data$Choice == 0, 'Nonpurchase', 'Purchase')
train_data$Choice <- as.factor(train_data$Choice)
levels(train_data$Choice) <- c('Nonpurchase', 'Purchase')

#ifelse(test_data$Gender == 0, 'Female', 'Male')
test_data$Gender <- as.factor(test_data$Gender)
levels(test_data$Gender) <- c('Female', 'Male')

#ifelse(test_data$Choice == 0, 'Nonpurchase', 'Purchase')
test_data$Choice <- as.factor(test_data$Choice)
levels(test_data$Choice) <-  c('Nonpurchase', 'Purchase')


train_data <- subset(train_data, select = -c(Observation))
test_data <- subset(test_data, select = -c(Observation))
concat_data <- rbind(train_data, test_data) 
sum(duplicated(concat_data))
concat_data <- unique(concat_data)
```


```{r}

# Step 5: Identify numeric columns for creating histograms
# Automatically identify numeric columns
numeric_columns <- sapply(concat_data, is.numeric)

# Step 6: Create histograms for numeric variables
# List to store plots
plots <- list()

# Loop through numeric columns to create histograms and add them to the list
for (colname in names(concat_data)[numeric_columns]) {
  p <- ggplot(train_data, aes_string(x = colname)) + 
        geom_histogram(bins = 30, fill = "blue", color = "black") + 
        ggtitle(paste("Histogram of", colname)) + 
        theme_minimal()
  plots[[colname]] <- p  # Store each plot in the list
}

# Step 7: Arrange the histograms in a grid layout
do.call("grid.arrange", c(plots, ncol = 2))  # Adjust ncol for the number of
```


```{r}
# Step 4: Subset the data to include only numeric variables
numeric_data <- concat_data[, sapply(concat_data, is.numeric)]

# Step 5: Calculate the correlation matrix
cor_matrix <- cor(numeric_data, use = "complete.obs")

# Step 6: Create a correlation plot
corrplot(cor_matrix, method = "circle", type = "upper", 
         tl.col = "black", tl.srt = 45, # Text color and rotation
         addCoef.col = "black")         # Add correlation coe
cor_matrix
```


```{r}
set.seed(1)
#preprocess_params <- preProcess(train_data[, -which(names(train_data) == "Choice")], method = c("center", "scale"))
#train <- predict(preprocess_params, train_data)
#test <- predict(preprocess_params, test_data)
train <- subset(concat_data, select = -c(Last_purchase))
str(train)
test <- subset(test_data, select = -c(Last_purchase))
str(test)

## 75% of the sample size
smp_size <- floor(0.7 * nrow(train))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(train)), size = smp_size)

train <- train[train_ind, ]
test <- train[-train_ind, ]

preprocess_params <- preProcess(train[, -which(names(train) == "Choice")], method = c("center", "scale"))

# Apply the preprocessing (scaling) to both training and testing sets
Train_scaled <- predict(preprocess_params, train)
Test_scaled <- predict(preprocess_params, test)

train <- Train_scaled
test <- Test_scaled
str(test)
```
```{r}
data_balanced_both <- ovun.sample(Choice ~ ., data =train, method = "both", p=0.5, seed = 1)$data 
data_balanced_both
```


```{r}
train_resample <- data_balanced_both
#train_resample <- downSample(x = train[, -which(names(train) == "Choice")], 
#                               y = train$Choice)
#colnames(train_resample)[ncol(train_resample)] <- "Choice"
#purchase = as.character(sum(train_resample$Choice == 1))
#nonpurchase = as.character(sum(train_resample$Choice == 0))
#paste0("Resampled purchase observations: ", purchase)
#paste0("Resampled nonpurchase observations: ", nonpurchase)
```

```{r}
set.seed(123)
folds = 10
repeats = 3
number = 5


# create model weights for training with imbalanced data 
model_weights = ifelse(train$Choice == 'Nonpurchase',
                       (1/table(train$Choice)[1]) * 0.5,
                       (1/table(train$Choice)[2])* 0.5)

# set trainControl parame
control <- trainControl(method = 'repeatedcv',
                        number = folds,
                        repeats = repeats,
                        classProbs = TRUE,
                        savePredictions = 'final')
```


```{r}
# Step 4: Train a Generalized Linear Model (GLM) for classification (logistic regression)
#control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
#test <- head(test,800)
my_models = c('glm', 'svmRadial', 'svmLinear', 'svmPoly')
all_models <- caretList(Choice ~ ., data = train, trControl = control, weights=model_weights, metric = "Accuracy", methodList = my_models, tuneList=list(glm=caretModelSpec(method='glm', family=quasibinomial)))
#glm_model <- train(Choice ~ Gender + Frequency + Last_purchase + P_Child + P_Youth + P_Cook + P_DIY + P_Art + Gender:Amount_purchased + Gender:P_Art + Amount_purchased:P_Child + Amount_purchased:P_Youth + Frequency:P_Youth + Frequency:P_Art + P_Child:P_Cook + P_Youth:P_Cook, data = train_resample, method = "glm", trControl = control)


list_of_results <- lapply(my_models, function(x) {all_models[[x]]$resample})

# convert to data frame
total_df <- do.call("bind_rows", list_of_results)
#total_df %<>% mutate(Model = lapply(my_models, function(x) {rep(x, folds*repeats)}) %>% unlist()) # create model names column next to their respective accuracy

predict_prob = function(model_select) {
    predict(model_select, test, type="prob") %>% # make predictions on model
    as.data.frame() %>% # convert to data frame
    return()
  
}
```

```{r}
pred_logit = predict_prob(all_models$glm)
pred_svm.lin = predict_prob(all_models$svmLinear)
pred_svm.rbf = predict_prob(all_models$svmRadial)
pred_svm.poly = predict_prob(all_models$svmPoly)

calc_auc = function(prob) {
  roc(test$Choice, prob)
}

auc_logit = calc_auc(pred_logit[,2])
auc_logit$auc[1] # we want this part included not the text so we index 
auc_svm.lin = calc_auc(pred_svm.lin[,2])
auc_svm.rbf = calc_auc(pred_svm.rbf[,2])
auc_svm.poly = calc_auc(pred_svm.poly[,2])
# create a data frame of AUC results for each model


df_auc = bind_rows(data_frame(TPR = auc_logit$sensitivities,
                              FPR = 1 - auc_logit$specificities,
                              AUC = auc_logit$auc[1],
                              Model = "Logistic"),
                    data_frame(TPR = auc_svm.lin$sensitivities,
                              FPR = 1 - auc_svm.lin$specificities,
                              AUC = auc_svm.lin$auc[1],
                              Model = "Linear SVM"),
                   data_frame(TPR = auc_svm.rbf$sensitivities,
                              FPR = 1 - auc_svm.rbf$specificities,
                              AUC = auc_svm.rbf$auc[1],
                              Model = "RBF SVM"),
                   data_frame(TPR = auc_svm.poly$sensitivities,
                              FPR = 1 - auc_svm.poly$specificities,
                              AUC = auc_svm.poly$auc[1],
                              Model = "Poly SVM"))

df_auc %>% 
  ggplot(aes(FPR, TPR, color=Model)) +
  geom_line(size = .5) +
  theme_bw() + 
  coord_equal() + 
  # dashed line indicates a model that is classifying poorly  
  geom_abline(intercept = 0, slope = 1, color='gray37', size=1, linetype="dashed") + 
  labs(x = "FPR (1 - Specificity)",
       y = "TPR (Sensitivity)",
       title = "ROC Curve and AUC: Model Comparison")

#glm_model$finalModel$coefficients
#vif(glm_model$finalModel)
```

```{r}
log_preds = predict(all_models$glm, test, type="raw")
confusionMatrix(as.factor(log_preds), as.factor(test$Choice), positive = "Purchase")

log_preds = predict(all_models$svmRadial, test, type="raw")
confusionMatrix(as.factor(log_preds), as.factor(test$Choice), positive = "Purchase")

log_preds = predict(all_models$svmPoly, test, type="raw")
confusionMatrix(as.factor(log_preds), as.factor(test$Choice), positive = "Purchase")

log_preds = predict(all_models$svmLinear, test, type="raw")
confusionMatrix(as.factor(log_preds), as.factor(test$Choice), positive = "Purchase")
car::vif(all_models$glm$finalModel)
```

```{r}
# Step 4: Train the SVM model with radial basis kernel (RBF)
set.seed(123)
svm_model <- svm(Choice ~ ., data = train, kernel = "radial", 
                 cost = 1, gamma = 0.1)  # Gamma is equivalent to sigma^2

# Step 5: Make predictions on the test set
svm_predictions <- predict(svm_model, newdata = test)

# Step 6: Evaluate the model using a confusion matrix
conf_matrix <- confusionMatrix(svm_predictions, test$Choice)

# Print the confusion matrix
print(conf_matrix)

# Step 7: Additional model performance metrics (optional)
summary(svm_model$decision.values)
```


```{r}

df_auc %>% 
  group_by(Model) %>% 
  summarize(Avg_Acc = max(Accuracy)) %>% 
  ungroup() %>% 
  arrange(-Avg_Acc) %>% 
  mutate_if(is.numeric, function(x) {round(x, 3)}) %>% 
  knitr::kable(caption = "Table 1: Model Performance in decreasing order of Accuracy",
               col.names = c("Model", "Accuracy"))
```


```{r}
par(mfrow=c(2,2))
plot(glm_model$finalModel, which=c(1:4)) 
#plot(glm_model$finalModel, which = 1)
# Check deviance and degrees of freedom
# Running Box-Tidwell test with `boxTidwell`
```


```{r}
# Predict on the test set
glm_model <- train(Choice ~ ., data = train, method = "glm", trControl = control)
glm_predictions <- predict(glm_model, test, type = "prob")
glm_prob <- glm_predictions[,2]
glm_class <- ifelse(glm_prob > 0.5, 1, 0)  # Convert to binary classes (0 or 1)
glm_class <- as.factor(glm_class)
levels(glm_class) <- levels(test$Choice)
# Evaluate the mode
confusionMatrix(test$Choice,glm_class, positive = "Purchase")
```

```{r}
roc_glm <- roc(as.numeric(test$Choice), glm_prob)
auc_glm <- auc(roc_glm)
#cat("Linear Regression AUC: ", auc_glm, "\n")
odds_ratio <- exp(coef(glm_model$finalModel))
```
```{r}
print(odds_ratio)
```

```{r}
varimport <- varImp(glm_model)
importance_df <- as.data.frame(varimport$importance)
importance_df$Variable <- rownames(importance_df)

# Create a plot using ggplot2
ggplot(importance_df, aes(x = reorder(Variable, Overall), y = Overall)) + 
  geom_bar(stat = "identity", fill = "steelblue") + 
  coord_flip() + 
  ggtitle("Variable Importance - Logistic Regression Model") + 
  xlab("Variables") + 
  ylab("Importance Score") +
  theme_minimal()

summary(glm_model$finalModel)
H <- data.frame(vif(glm_model$finalModel))
H

```


```{r}
# Step 4: Train a Generalized Linear Model (GLM) for classification (logistic regression)
set.seed(123)
control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
lm_model <- train(as.numeric(Choice) ~ ., data = train_resample, method = "lm", trControl = control)

# Step 6: Make predictions on the test set
lm_predictions <- predict(lm_model, test)

# Step 7: Convert continuous predictions to bi nary (0 or 1) using a threshold (e.g., 0.5)
lm_class <- ifelse(lm_predictions > 0.5, 1, 0)
lm_class <- as.factor(lm_class)
levels(lm_class) <- levels(test$Choice)

# Step 9: Calculate the confusion matrix
conf_matrix <- confusionMatrix(lm_class, as.factor(test$Choice), positive = "Purchase")
summary(lm_model)
conf_matrix
lm_model$bestTune
```


```{r}
# Predict on the test set
lm_predictions <- predict(lm_model, test)
lm_class <- ifelse(lm_predictions > 0.5, 1, 0)  # Convert to binary classes (0 or 1)
lm_class <- as.factor(lm_class)
levels(lm_class) <- levels(test$Choice)
# Evaluate the model
confusionMatrix(as.factor(lm_class), as.factor(test$Choice), positive = "Purchase")
```

```{r}
lm_prob <- lm_predictions
roc_lm <- roc(as.numeric(test$Choice), lm_prob)
auc_lm <- auc(roc_lm)
#cat("Linear Regression AUC: ", auc_glm, "\n")
```

```{r}
#form1 = Choice ~.
set.seed(1)
form1 = Choice ~ .
radial_tune = tune.svm(form1, data=train_resample, gamma = seq(0.1, .1, by= 0.01), cost = seq(.1,1, by = .1))
radial_svm = svm(formula = form1, data = train_resample, gamma = radial_tune$best.parameters$gamma, cost = radial_tune$best.parameters$cost, decision.values=TRUE, probability=TRUE, kernel = "radial")
summary(radial_svm)
radial_tune$best.parameters
#print(radial_svm$decision.values)
```

```{r}
radial_predictions = predict(radial_svm, test, type="response", probability = TRUE, decision.values = TRUE)
caret::confusionMatrix(test$Choice, as.factor(radial_predictions), positive = "Purchase")
#svm_class <- ifelse(svmpredict > 0.5, 1, 0)  # Convert to binary classes (0 or 1)
#svm_class <- as.factor(svm_class)
#levels(svm_class) <- levels(test$Choice)
# Evaluate the model
#caret::confusionMatrix(as.factor(svmpredict), as.factor(test$Choice), positive = "1")
```

```{r}
bestsvm_radialprob <- attr(radial_predictions, "probabilities")
roc_radial <- roc(as.numeric(test$Choice), as.numeric(bestsvm_radialprob[,1]))
auc_radial <- auc(roc_radial)
table(pred=radial_predictions, true=test$Choice)
```


```{r}
poly_tune = tune.svm(form1, data=train_resample, gamma = seq(0.1, .1, by= 0.01), cost = seq(.1,1, by = .1))
poly_svm = svm(formula = form1, data = train_resample, gamma = poly_tune$best.parameters$gamma, cost = poly_tune$best.parameters$cost, decision.values=TRUE, probability=TRUE, kernel = "poly")
summary(poly_svm)
poly_tune$best.parameters
```

```{r}
poly_predictions = predict(poly_svm, test, type="response", probability = TRUE, decision.values = TRUE)
caret::confusionMatrix(test$Choice, as.factor(poly_predictions), positive = "Purchase")
#svm_class <- ifelse(svmpredict > 0.5, 1, 0)  # Convert to binary classes (0 or 1)
#svm_class <- as.factor(svm_class)
#levels(svm_class) <- levels(test$Choice)
# Evaluate the model
#caret::confusionMatrix(as.factor(svmpredict), as.factor(test$Choice), positive = "1")
```

```{r}
bestsvm_polyprob <- attr(poly_predictions, "probabilities")
roc_poly <- roc(as.numeric(test$Choice), as.numeric(bestsvm_polyprob[,1]))
auc_poly <- auc(roc_poly)
table(pred=poly_predictions, true=test$Choice)
```


```{r}
set.seed(1)
sigmoid_tune = tune.svm(form1, data = train_resample ,kernel = "sigmoid", gamma = seq(.01, .1, by = .01), cost = seq(.1, 1, by = .1))
sigmoid_svm = svm(formula = form1, data = train_resample,kernel = "sigmoid", gamma = sigmoid_tune$best.parameters$gamma, cost = sigmoid_tune$best.parameters$cost, probability = TRUE, decision.values = TRUE)
summary(sigmoid_svm)
```

```{r}
sigmoid_predictions = predict(sigmoid_svm, test, type = "response", probability = TRUE, decision.values = TRUE)
caret::confusionMatrix(test$Choice, as.factor(sigmoid_predictions), positive = "Purchase")
```

```{r}
bestsvm_sidmoidprob <- attr(sigmoid_predictions, "probabilities")
roc_sigmoid <- roc(as.numeric(test$Choice), as.numeric(bestsvm_sidmoidprob[,1]))
auc_sigmoid <- auc(roc_sigmoid)
table(pred=sigmoid_predictions, true=test$Choice)
```
```{r}
linear_tune = tune.svm(form1, data=train_resample, gamma = seq(0.1, .1, by= 0.01), cost = seq(.1,1, by = .1))
linear_svm = svm(formula = form1, data = train_resample, gamma = linear_tune$best.parameters$gamma, cost = linear_tune$best.parameters$cost, decision.values=TRUE, probability=TRUE, kernel = "linear")
summary(linear_svm)
```

```{r}
linear_predictions = predict(linear_svm, test, type="response", probability = TRUE, decision.values = TRUE)
caret::confusionMatrix(test$Choice, as.factor(linear_predictions), positive = "Purchase")
#svm_class <- ifelse(svmpredict > 0.5, 1, 0)  # Convert to binary classes (0 or 1)
#svm_class <- as.factor(svm_class)
#levels(svm_class) <- levels(test$Choice)
# Evaluate the model
#caret::confusionMatrix(as.factor(svmpredict), as.factor(test$Choice), positive = "1")
```

```{r}
bestsvm_linearprob <- attr(linear_predictions, "probabilities")
roc_linear <- roc(as.numeric(test$Choice), as.numeric(bestsvm_linearprob[,1]))
auc_linear <- auc(roc_linear)
table(pred=linear_predictions, true=test$Choice)
```

```{r}
lm_matrix <- confusionMatrix(as.factor(test$Choice), as.factor(lm_class), positive = "Purchase")
lm_accuracy <- lm_matrix$overall[1]
lm_accuracy

glm_matrix <- confusionMatrix(as.factor(test$Choice),as.factor(glm_class), positive = "Purchase")
glm_accuracy <- glm_matrix$overall[1]
glm_accuracy


radial_matrix <- confusionMatrix(as.factor(test$Choice),as.factor(radial_predictions), positive = "Purchase")
radial_accuracy <- radial_matrix$overall[1]
radial_accuracy

poly_matrix <- confusionMatrix(as.factor(test$Choice),as.factor(poly_predictions), positive = "Purchase")
poly_accuracy <- poly_matrix$overall[1]
poly_accuracy

sigmoid_matrix <- confusionMatrix(as.factor(test$Choice),as.factor(sigmoid_predictions), positive = "Purchase")
sigmoid_accuracy <- sigmoid_matrix$overall[1]
sigmoid_accuracy

linear_matrix <- confusionMatrix(as.factor(test$Choice),as.factor(linear_predictions), positive = "Purchase")
linear_accuracy <- linear_matrix$overall[1]
linear_accuracy
```
```{r}
radial_matrix
```

```{r}
evaluation_metrics <- function(model_auc, model_matrix, model_name) {
  # Extract values from the confusion matrix for test data
  TN_test <- model_matrix$table[1,1]  # True Negatives
  FP_test <- model_matrix$table[1,2]  # False Positives
  FN_test <- model_matrix$table[2,1]  # False Negatives
  TP_test <- model_matrix$table[2,2]  # True Positives
  
  # Calculate accuracy for the test data
  accuracy_test <- (TP_test + TN_test) / sum(model_matrix$table)
  
  # Calculate precision, recall, and F1 score for the test data
  precision_test <- ifelse((TP_test + FP_test) > 0, TP_test / (TP_test + FP_test), 0)
  recall_test <- ifelse((TP_test + FN_test) > 0, TP_test / (TP_test + FN_test), 0)
  f1_score_test <- ifelse((precision_test + recall_test) > 0, 
                          2 * ((precision_test * recall_test) / (precision_test + recall_test)), 
                          0)
  metric <- c("Model", "Accuracy", "Precision", "Recall", "F1_Score")
  #metric <- c("Accuracy", "Precision", "Recall", "F1_Score")
  metric_values <- c(model_name, accuracy_test, precision_test, recall_test, f1_score_test)
  model_performance <-data.frame(
    Model=model_name,
    AUC=model_auc,
    Accuracy=accuracy_test,
    Precision=precision_test,
    Recall=recall_test,
    F1_score=f1_score_test)
  return (model_performance)
  # Print the performance metrics for the test data
  #print(paste("Accuracy (Test):", accuracy_test))
  #print(paste("Precision (Test):", precision_test))
  #print(paste("Recall (Test):", recall_test))
  #print(paste("F1 Score (Test):", f1_score_test))
}
```

```{r}
glm <- evaluation_metrics(auc_glm, glm_matrix, "Logistic Regression")
lm <- evaluation_metrics(auc_lm, lm_matrix, "Linear Regression")
radial <- evaluation_metrics(auc_radial, radial_matrix, "RBF SVM")
polys <- evaluation_metrics(auc_poly, poly_matrix, "Polynomial SVM")
linearsvm <- evaluation_metrics(auc_linear, linear_matrix, "Linear S VM")
sigmoids <- evaluation_metrics(auc_sigmoid, sigmoid_matrix, "Sigmoid SVM")
```

```{r}
models_performance_metrics <- rbind(lm,glm,radial,polys,linearsvm,sigmoids)
final_metrics <- models_performance_metrics[order(models_performance_metrics$Accuracy, decreasing = TRUE),]
final_metrics
```


```{r}
max_metrics <- function(perf_metrics_df, max_metric) {
  testing <- perf_metrics_df %>% 
    slice(which.max(perf_metrics_df[[max_metric]])) %>%
    select("Model",max_metric)
  return(testing)
}

```

```{r}
#col_list <- colnames(models_performance_metrics)
#for(i in col_list) {
#  if (i=="AUC") {
#    tests <- models_performance_metrics %>%
#      slice(which.max(models_performance_metrics[[i]])) %>%
#      select(Model, i)
#    print(tests)
#  }
#  else {print("Not metric")}
#}
auc_max <- max_metrics(models_performance_metrics, "AUC")
accuracy_max <- max_metrics(models_performance_metrics, "Accuracy")
precision_max <- max_metrics(models_performance_metrics, "Precision")
recall_max <- max_metrics(models_performance_metrics, "Recall")
f1_max <- max_metrics(models_performance_metrics, "F1_score")
f1_max
```




```{r}
# create a data frame of AUC results for each model
df_auc = bind_rows(data_frame(TPR = roc_lm$sensitivities,
                              FPR = 1 - roc_lm$specificities,
                              AUC = auc_lm[1],
                              Accuracy = lm_accuracy,
                              Model = "Linear Regression"),
                   data_frame(TPR = roc_glm$sensitivities,
                              FPR = 1 - roc_glm$specificities,
                              AUC = auc_glm[1],
                              Accuracy = glm_accuracy,                  
                              Model = "Logistic Regression"),
                   data_frame(TPR = roc_radial$sensitivities,
                              FPR = 1 - roc_radial$specificities,
                              AUC = auc_radial[1],
                              Accuracy = radial_accuracy,
                              Model = "Radial SVM"),
                   data_frame(TPR = roc_poly$sensitivities,
                              FPR = 1 - roc_poly$specificities,
                              AUC = auc_poly[1],
                              Accuracy = poly_accuracy,                  
                                Model = "Poly SVM"),
                   data_frame(TPR = roc_sigmoid$sensitivities,
                              FPR = 1 - roc_sigmoid$specificities,
                              AUC = auc_sigmoid[1],
                              Accuracy = sigmoid_accuracy,                  
                              Model = "Sigmoid SVM"),
                   data_frame(TPR = roc_linear$sensitivities,
                              FPR = 1 - roc_linear$specificities,
                              AUC = auc_linear[1],
                              Accuracy = linear_accuracy,                  
                              Model = "Linear SVM"))
df_auc
```

```{r}
# Plot ROC curves to compare each model
# The more L-shaped the curve, the better

df_auc %>% 
  ggplot(aes(FPR, TPR, color=Model)) +
  geom_line(size = .5) +
  theme_bw() + 
  coord_equal() + 
  # dashed line indicates a model that is classifying poorly  
  geom_abline(intercept = 0, slope = 1, color='gray37', size=1, linetype="dashed") + 
  labs(x = "FPR (1 - Specificity)",
       y = "TPR (Sensitivity)",
       title = "ROC Curve and AUC: Model Comparison")
```


```{r}
# Show which models had the highest average accuracy
# Show which models had the highest average accuracy

df_auc %>% 
  group_by(Model) %>% 
  summarize(Avg_Acc = max(Accuracy)) %>% 
  ungroup() %>% 
  arrange(-Avg_Acc) %>% 
  mutate_if(is.numeric, function(x) {round(x, 3)}) %>% 
  knitr::kable(caption = "Table 1: Model Performance in decreasing order of Accuracy",
               col.names = c("Model", "Accuracy"))
```




```{r}
# Show which models had the best AUC

df_auc %>% 
  group_by(Model) %>% 
  summarize(AUC = max(AUC)) %>% 
  ungroup() %>% 
  arrange(-AUC) %>% 
  mutate_if(is.numeric, function(x) {round(x, 4)}) %>% 
  knitr::kable(caption = "Table 2: Model Performance in decreasing order of AUC",
               col.names = c("Model", "AUC"))
```

```{r}
# Show which models had the best Specificity (aka TPR)

df_auc %>% 
  group_by(Model) %>% 
  summarize(TPR = mean(TPR)) %>% 
  ungroup() %>% 
  arrange(-TPR) %>% 
  mutate_if(is.numeric, function(x) {round(x, 4)}) %>% 
  knitr::kable(caption = "Table 3: Model Performance in decreasing order of Sensitivity",
               col.names = c("Model", "TPR"))
```

```{r}
# Show which models had the best Specificity (aka FPR)

df_auc %>% 
  group_by(Model) %>% 
  summarize(FPR = mean(FPR)) %>% 
  ungroup() %>% 
  arrange(-FPR) %>% 
  mutate_if(is.numeric, function(x) {round(x, 4)}) %>% 
  knitr::kable(caption = "Table 4: Model Performance in decreasing order of Specificity",
               col.names = c("Model", "FPR"))
```

```{r}
linear_matrix 
radial_matrix
sigmoid_matrix
lm_matrix
glm_matrix
```

```{r}
xtab <- table(log_preds, test$Choice)
xtab
```


```{r}
tp_prop <- 689/824 # nonpurchasers predicted
fn_prop <- 104/824 # predicted as nonpurchase, but purchased
fp_prop <- 5/824 # predicted purchase, did not purchase
tn_prop <- 26/824 # predicted purchase and purchased

cust_opportunity <- 50000
```

```{r}
tp <- tp_prop*cust_opportunity
fn <- fp_prop*cust_opportunity
fp <- fn_prop*cust_opportunity
tn <- tn_prop*cust_opportunity
tn
```

```{r}
total_ads <- fn + tn
successful_ad_purchases_pct <- tn / total_ads
successful_ad_purchases_pct
```

```{r}
total_mailing_cost <- total_ads *.65
total_books_cogs <- total_ads * 15
total_overhead_cost <- total_books_cogs*.45
total_revenue <- tn*31.95
total_profit <- total_revenue - total_overhead_cost - total_books_cogs - total_mailing_cost
total_profit
```

```{r}
# 130 purchased
# 694 no purchase

purch_prop <- 130/824
no_purch_prop <- 694/824

total_purch_cust <- purch_prop*50000
total_no_purch_cust <- no_purch_prop*50000

prop_sent_total <- total_purch_cust/50000
prop_sent_total
```

```{r}
total_mailing_cost_all <- 50000*.65
book_buy_all <- total_purch_cust*15
cost_overhead_all <- book_buy_all*.45
revenue_total_all <- 31.95*total_purch_cust
profit_total_all <- revenue_total_all - cost_overhead_all - book_buy_all - total_mailing_cost_all
profit_total_all
```

