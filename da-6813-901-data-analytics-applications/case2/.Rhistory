# 130 purchased
# 694 no purchase
purch_prop <- 130/824
no_purch_prop <- 694/824
total_purch_cust <- purch_prop*50000
total_no_purch_cust <- no_purch_prop*50000
prop_sent_total <- total_purch_cust/50000
prop_sent_total
total_mailing_cost_all <- 50000*.65
book_buy_all <- total_purch_cust*15
cost_overhead_all <- book_buy_all*.45
revenue_total_all <- 31.95*total_purch_cust
profit_total_all <- revenue_total_all - cost_overhead_all - book_buy_all - total_mailing_cost_all
profit_total_all
total_mailing_cost_all <- 50000*.65
book_buy_all <- total_purch_cust*15
cost_overhead_all <- book_buy_all*.45
revenue_total_all <- 31.95*total_purch_cust
profit_total_all <- revenue_total_all - cost_overhead_all - book_buy_all - total_mailing_cost_all
profit_total_all
total_mailing_cost_all
total_mailing_cost_all <- 50000*.65
book_buy_all <- total_purch_cust*15
cost_overhead_all <- book_buy_all*.45
revenue_total_all <- 31.95*total_purch_cust
profit_total_all <- revenue_total_all - cost_overhead_all - book_buy_all - total_mailing_cost_all
profit_total_all
total_mailing_cost_all
book_buy_all
total_mailing_cost_all <- 50000*.65
book_buy_all <- total_purch_cust*15
cost_overhead_all <- book_buy_all*.45
revenue_total_all <- 31.95*total_purch_cust
profit_total_all <- revenue_total_all - cost_overhead_all - book_buy_all - total_mailing_cost_all
profit_total_all
total_mailing_cost_all
book_buy_all
cost_overhead_all
total_mailing_cost_all <- 50000*.65
book_buy_all <- total_purch_cust*15
cost_overhead_all <- book_buy_all*.45
revenue_total_all <- 31.95*total_purch_cust
profit_total_all <- revenue_total_all - cost_overhead_all - book_buy_all - total_mailing_cost_all
profit_total_all
total_mailing_cost_all
book_buy_all
cost_overhead_all
revenue_total_all
total_mailing_cost <- total_ads *.65
total_books_cogs <- total_ads * 15
total_overhead_cost <- total_books_cogs*.45
total_revenue <- tn*31.95
total_profit <- total_revenue - total_overhead_cost - total_books_cogs - total_mailing_cost
total_profit
total_mailing_cost_all <- 50000*.65
book_buy_all <- total_purch_cust*15
cost_overhead_all <- book_buy_all*.45
revenue_total_all <- 31.95*total_purch_cust
profit_total_all <- revenue_total_all - cost_overhead_all - book_buy_all - total_mailing_cost_all
profit_total_all
par(mfrow=c(2,2))
#plot(all_models$svmRadial$finalModel, which=c(1:4))
#plot(glm_model$finalModel, which = 1)
# Check deviance and degrees of freedom
# Running Box-Tidwell test with `boxTidwell`
all_models$svmRadial$bestTune
extractMetric(all_models$svmRadial)
# Predict on the test set
glm_predictions <- predict(glm_model, test, type = "prob")
glm_prob <- glm_predictions[,2]
glm_class <- ifelse(glm_prob > 0.5, 1, 0)  # Convert to binary classes (0 or 1)
glm_class <- as.factor(glm_class)
levels(glm_class) <- levels(test$Choice)
# Evaluate the mode
confusionMatrix(test$Choice,glm_class, positive = "Purchase")
roc_glm <- roc(as.numeric(test$Choice), glm_prob)
auc_glm <- auc(roc_glm)
#cat("Linear Regression AUC: ", auc_glm, "\n")
odds_ratio <- exp(coef(glm_model$finalModel))
print(odds_ratio)
varimport <- varImp(glm_model)
importance_df <- as.data.frame(varimport$importance)
importance_df$Variable <- rownames(importance_df)
# Create a plot using ggplot2
ggplot(importance_df, aes(x = reorder(Variable, Overall), y = Overall)) +
geom_bar(stat = "identity", fill = "steelblue") +
coord_flip() +
ggtitle("Variable Importance - Logistic Regression Model") +
xlab("Variables") +
ylab("Importance Score") +
theme_minimal()
summary(glm_model$finalModel)
vif(glm_model$finalModel)
# Step 4: Train a Generalized Linear Model (GLM) for classification (logistic regression)
set.seed(123)
control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
lm_model <- train(as.numeric(Choice) ~ + Frequency + First_purchase + P_Child + P_Cook + P_DIY + P_Art + Amount_purchased:P_Youth, data = train_resample, method = "lm", trControl = control)
# Step 6: Make predictions on the test set
lm_predictions <- predict(lm_model, test)
# Step 7: Convert continuous predictions to binary (0 or 1) using a threshold (e.g., 0.5)
lm_class <- ifelse(lm_predictions > 0.5, 1, 0)
lm_class <- as.factor(lm_class)
levels(lm_class) <- levels(test$Choice)
# Step 9: Calculate the confusion matrix
conf_matrix <- confusionMatrix(lm_class, as.factor(test$Choice), positive = "1")
# Step 4: Train a Generalized Linear Model (GLM) for classification (logistic regression)
set.seed(123)
control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
lm_model <- train(as.numeric(Choice) ~ ., data = train_resample, method = "lm", trControl = control)
# Step 6: Make predictions on the test set
lm_predictions <- predict(lm_model, test)
# Step 7: Convert continuous predictions to binary (0 or 1) using a threshold (e.g., 0.5)
lm_class <- ifelse(lm_predictions > 0.5, 1, 0)
lm_class <- as.factor(lm_class)
levels(lm_class) <- levels(test$Choice)
# Step 9: Calculate the confusion matrix
conf_matrix <- confusionMatrix(lm_class, as.factor(test$Choice), positive = "1")
# Step 4: Train a Generalized Linear Model (GLM) for classification (logistic regression)
set.seed(123)
control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
lm_model <- train(as.numeric(Choice) ~ ., data = train_resample, method = "lm")
# Step 6: Make predictions on the test set
lm_predictions <- predict(lm_model, test)
# Step 7: Convert continuous predictions to binary (0 or 1) using a threshold (e.g., 0.5)
lm_class <- ifelse(lm_predictions > 0.5, 1, 0)
lm_class <- as.factor(lm_class)
levels(lm_class) <- levels(test$Choice)
# Step 9: Calculate the confusion matrix
conf_matrix <- confusionMatrix(lm_class, as.factor(test$Choice), positive = "1")
# Step 4: Train a Generalized Linear Model (GLM) for classification (logistic regression)
set.seed(123)
control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
lm_model <- train(as.numeric(Choice) ~ ., data = train_resample, method = "lm", trControl = control)
# Step 6: Make predictions on the test set
lm_predictions <- predict(lm_model, test)
# Step 7: Convert continuous predictions to binary (0 or 1) using a threshold (e.g., 0.5)
lm_class <- ifelse(lm_predictions > 0.5, 1, 0)
lm_class <- as.factor(lm_class)
levels(lm_class) <- levels(test$Choice)
# Step 9: Calculate the confusion matrix
conf_matrix <- confusionMatrix(lm_class, as.factor(test$Choice), positive = "Purchase")
conf_matrix
# Predict on the test set
lm_predictions <- predict(lm_model, test)
lm_class <- ifelse(lm_predictions > 0.5, 1, 0)  # Convert to binary classes (0 or 1)
lm_class <- as.factor(lm_class)
levels(lm_class) <- levels(test$Choice)
# Evaluate the model
confusionMatrix(as.factor(lm_class), as.factor(test$Choice), positive = "Purchase")
lm_prob <- lm_predictions
roc_lm <- roc(as.numeric(test$Choice), lm_prob)
auc_lm <- auc(roc_lm)
#cat("Linear Regression AUC: ", auc_glm, "\n")
#form1 = Choice ~.
set.seed(1)
form1 = Choice ~ .
radial_tune = tune.svm(form1, data=train_resample, gamma = seq(0.1, .1, by= 0.01), cost = seq(.1,1, by = .1))
radial_svm = svm(formula = form1, data = train_resample, gamma = radial_tune$best.parameters$gamma, cost = radial_tune$best.parameters$cost, decision.values=TRUE, probability=TRUE, kernel = "radial")
#summary(radial_svm)
radial_tune$best.parameters
radial_predictions = predict(radial_svm, test, type="response", probability = TRUE, decision.values = TRUE)
caret::confusionMatrix(test$Choice, as.factor(radial_predictions), positive = "1")
bestsvm_radialprob <- attr(radial_predictions, "probabilities")
roc_radial <- roc(as.numeric(test$Choice), as.numeric(bestsvm_radialprob[,1]))
auc_radial <- auc(roc_radial)
table(pred=radial_predictions, true=test$Choice)
poly_tune = tune.svm(form1, data=train_resample, gamma = seq(0.1, .1, by= 0.01), cost = seq(.1,1, by = .1))
poly_svm = svm(formula = form1, data = train_resample, gamma = poly_tune$best.parameters$gamma, cost = poly_tune$best.parameters$cost, decision.values=TRUE, probability=TRUE, kernel = "poly")
summary(poly_svm)
poly_predictions = predict(poly_svm, test, type="response", probability = TRUE, decision.values = TRUE)
caret::confusionMatrix(test$Choice, as.factor(poly_predictions), positive = "1")
bestsvm_polyprob <- attr(poly_predictions, "probabilities")
roc_poly <- roc(as.numeric(test$Choice), as.numeric(bestsvm_polyprob[,1]))
auc_poly <- auc(roc_poly)
table(pred=poly_predictions, true=test$Choice)
set.seed(1)
sigmoid_tune = tune.svm(form1, data = train_resample ,kernel = "sigmoid", gamma = seq(.01, .1, by = .01), cost = seq(.1, 1, by = .1))
sigmoid_svm = svm(formula = form1, data = train_resample,kernel = "sigmoid", gamma = sigmoid_tune$best.parameters$gamma, cost = sigmoid_tune$best.parameters$cost, probability = TRUE, decision.values = TRUE)
summary(sigmoid_svm)
sigmoid_predictions = predict(sigmoid_svm, test, type = "response", probability = TRUE, decision.values = TRUE)
caret::confusionMatrix(test$Choice, as.factor(sigmoid_predictions), positive = "1")
sigmoid_predictions = predict(sigmoid_svm, test, type = "response", probability = TRUE, decision.values = TRUE)
caret::confusionMatrix(test$Choice, as.factor(sigmoid_predictions), positive = "Purchase")
bestsvm_sidmoidprob <- attr(sigmoid_predictions, "probabilities")
roc_sigmoid <- roc(as.numeric(test$Choice), as.numeric(bestsvm_sidmoidprob[,1]))
auc_sigmoid <- auc(roc_sigmoid)
table(pred=sigmoid_predictions, true=test$Choice)
linear_tune = tune.svm(form1, data=train_resample, gamma = seq(0.1, .1, by= 0.01), cost = seq(.1,1, by = .1))
linear_svm = svm(formula = form1, data = train_resample, gamma = linear_tune$best.parameters$gamma, cost = linear_tune$best.parameters$cost, decision.values=TRUE, probability=TRUE, kernel = "linear")
summary(linear_svm)
sigmoid_predictions = predict(sigmoid_svm, test, type = "response", probability = TRUE, decision.values = TRUE)
caret::confusionMatrix(test$Choice, as.factor(sigmoid_predictions), positive = "Purchase")
bestsvm_sidmoidprob <- attr(sigmoid_predictions, "probabilities")
roc_sigmoid <- roc(as.numeric(test$Choice), as.numeric(bestsvm_sidmoidprob[,1]))
auc_sigmoid <- auc(roc_sigmoid)
table(pred=sigmoid_predictions, true=test$Choice)
linear_tune = tune.svm(form1, data=train_resample, gamma = seq(0.1, .1, by= 0.01), cost = seq(.1,1, by = .1))
linear_svm = svm(formula = form1, data = train_resample, gamma = linear_tune$best.parameters$gamma, cost = linear_tune$best.parameters$cost, decision.values=TRUE, probability=TRUE, kernel = "linear")
summary(linear_svm)
poly_predictions = predict(poly_svm, test, type="response", probability = TRUE, decision.values = TRUE)
caret::confusionMatrix(test$Choice, as.factor(poly_predictions), positive = "Purchase")
#svm_class <- ifelse(svmpredict > 0.5, 1, 0)  # Convert to binary classes (0 or 1)
#svm_class <- as.factor(svm_class)
#levels(svm_class) <- levels(test$Choice)
# Evaluate the model
#caret::confusionMatrix(as.factor(svmpredict), as.factor(test$Choice), positive = "1")
radial_predictions = predict(radial_svm, test, type="response", probability = TRUE, decision.values = TRUE)
caret::confusionMatrix(test$Choice, as.factor(radial_predictions), positive = "Purchase")
#svm_class <- ifelse(svmpredict > 0.5, 1, 0)  # Convert to binary classes (0 or 1)
#svm_class <- as.factor(svm_class)
#levels(svm_class) <- levels(test$Choice)
# Evaluate the model
#caret::confusionMatrix(as.factor(svmpredict), as.factor(test$Choice), positive = "1")
bestsvm_radialprob <- attr(radial_predictions, "probabilities")
roc_radial <- roc(as.numeric(test$Choice), as.numeric(bestsvm_radialprob[,1]))
auc_radial <- auc(roc_radial)
table(pred=radial_predictions, true=test$Choice)
poly_tune = tune.svm(form1, data=train_resample, gamma = seq(0.1, .1, by= 0.01), cost = seq(.1,1, by = .1))
poly_svm = svm(formula = form1, data = train_resample, gamma = poly_tune$best.parameters$gamma, cost = poly_tune$best.parameters$cost, decision.values=TRUE, probability=TRUE, kernel = "poly")
summary(poly_svm)
poly_predictions = predict(poly_svm, test, type="response", probability = TRUE, decision.values = TRUE)
caret::confusionMatrix(test$Choice, as.factor(poly_predictions), positive = "Purchase")
#svm_class <- ifelse(svmpredict > 0.5, 1, 0)  # Convert to binary classes (0 or 1)
#svm_class <- as.factor(svm_class)
#levels(svm_class) <- levels(test$Choice)
# Evaluate the model
#caret::confusionMatrix(as.factor(svmpredict), as.factor(test$Choice), positive = "1")
bestsvm_polyprob <- attr(poly_predictions, "probabilities")
roc_poly <- roc(as.numeric(test$Choice), as.numeric(bestsvm_polyprob[,1]))
auc_poly <- auc(roc_poly)
table(pred=poly_predictions, true=test$Choice)
set.seed(1)
sigmoid_tune = tune.svm(form1, data = train_resample ,kernel = "sigmoid", gamma = seq(.01, .1, by = .01), cost = seq(.1, 1, by = .1))
sigmoid_svm = svm(formula = form1, data = train_resample,kernel = "sigmoid", gamma = sigmoid_tune$best.parameters$gamma, cost = sigmoid_tune$best.parameters$cost, probability = TRUE, decision.values = TRUE)
summary(sigmoid_svm)
sigmoid_svm = svm(formula = form1, data = train_resample,kernel = "sigmoid", gamma = sigmoid_tune$best.parameters$gamma, cost = sigmoid_tune$best.parameters$cost, probability = TRUE, decision.values = TRUE)
bestsvm_sidmoidprob <- attr(sigmoid_predictions, "probabilities")
roc_sigmoid <- roc(as.numeric(test$Choice), as.numeric(bestsvm_sidmoidprob[,1]))
auc_sigmoid <- auc(roc_sigmoid)
table(pred=sigmoid_predictions, true=test$Choice)
linear_tune = tune.svm(form1, data=train_resample, gamma = seq(0.1, .1, by= 0.01), cost = seq(.1,1, by = .1))
linear_svm = svm(formula = form1, data = train_resample, gamma = linear_tune$best.parameters$gamma, cost = linear_tune$best.parameters$cost, decision.values=TRUE, probability=TRUE, kernel = "linear")
summary(linear_svm)
linear_predictions = predict(linear_svm, test, type="response", probability = TRUE, decision.values = TRUE)
caret::confusionMatrix(test$Choice, as.factor(linear_predictions), positive = "1")
linear_predictions = predict(linear_svm, test, type="response", probability = TRUE, decision.values = TRUE)
caret::confusionMatrix(test$Choice, as.factor(linear_predictions), positive = "Purchase")
#svm_class <- ifelse(svmpredict > 0.5, 1, 0)  # Convert to binary classes (0 or 1)
#svm_class <- as.factor(svm_class)
#levels(svm_class) <- levels(test$Choice)
# Evaluate the model
#caret::confusionMatrix(as.factor(svmpredict), as.factor(test$Choice), positive = "1")
bestsvm_linearprob <- attr(linear_predictions, "probabilities")
roc_linear <- roc(as.numeric(test$Choice), as.numeric(bestsvm_linearprob[,1]))
auc_linear <- auc(roc_linear)
table(pred=linear_predictions, true=test$Choice)
lm_matrix <- confusionMatrix(as.factor(test$Choice), as.factor(lm_class), positive = "1")
lm_matrix <- confusionMatrix(as.factor(test$Choice), as.factor(lm_class), positive = "Purchase")
lm_accuracy <- lm_matrix$overall[1]
lm_accuracy
glm_matrix <- confusionMatrix(as.factor(test$Choice),as.factor(glm_class), positive = "Purchase")
glm_accuracy <- glm_matrix$overall[1]
glm_accuracy
radial_matrix <- confusionMatrix(as.factor(test$Choice),as.factor(radial_predictions), positive = "Purchase")
radial_accuracy <- radial_matrix$overall[1]
radial_accuracy
poly_matrix <- confusionMatrix(as.factor(test$Choice),as.factor(poly_predictions), positive = "Purchase")
poly_accuracy <- poly_matrix$overall[1]
poly_accuracy
sigmoid_matrix <- confusionMatrix(as.factor(test$Choice),as.factor(sigmoid_predictions), positive = "Purchase")
sigmoid_accuracy <- sigmoid_matrix$overall[1]
sigmoid_accuracy
linear_matrix <- confusionMatrix(as.factor(test$Choice),as.factor(linear_predictions), positive = "Purchase")
linear_accuracy <- linear_matrix$overall[1]
linear_accuracy
evaluation_metrics <- function(model_auc, model_matrix, model_name) {
# Extract values from the confusion matrix for test data
TN_test <- model_matrix$table[1,1]  # True Negatives
FP_test <- model_matrix$table[1,2]  # False Positives
FN_test <- model_matrix$table[2,1]  # False Negatives
TP_test <- model_matrix$table[2,2]  # True Positives
# Calculate accuracy for the test data
accuracy_test <- (TP_test + TN_test) / sum(model_matrix$table)
# Calculate precision, recall, and F1 score for the test data
precision_test <- ifelse((TP_test + FP_test) > 0, TP_test / (TP_test + FP_test), 0)
recall_test <- ifelse((TP_test + FN_test) > 0, TP_test / (TP_test + FN_test), 0)
f1_score_test <- ifelse((precision_test + recall_test) > 0,
2 * ((precision_test * recall_test) / (precision_test + recall_test)),
0)
metric <- c("Model", "Accuracy", "Precision", "Recall", "F1_Score")
#metric <- c("Accuracy", "Precision", "Recall", "F1_Score")
metric_values <- c(model_name, accuracy_test, precision_test, recall_test, f1_score_test)
model_performance <-data.frame(
Model=model_name,
AUC=model_auc,
Accuracy=accuracy_test,
Precision=precision_test,
Recall=recall_test,
F1_score=f1_score_test)
return (model_performance)
# Print the performance metrics for the test data
#print(paste("Accuracy (Test):", accuracy_test))
#print(paste("Precision (Test):", precision_test))
#print(paste("Recall (Test):", recall_test))
#print(paste("F1 Score (Test):", f1_score_test))
}
glm <- evaluation_metrics(auc_glm, glm_matrix, "Logistic Regression")
lm <- evaluation_metrics(auc_lm, lm_matrix, "Linear Regression")
radial <- evaluation_metrics(auc_radial, radial_matrix, "RBF SVM")
polys <- evaluation_metrics(auc_poly, poly_matrix, "Polynomial SVM")
linearsvm <- evaluation_metrics(auc_linear, linear_matrix, "Linear SVM")
sigmoids <- evaluation_metrics(auc_sigmoid, sigmoid_matrix, "Sigmoid SVM")
models_performance_metrics <- rbind(lm,glm,radial,polys,linearsvm,sigmoids)
final_metrics <- models_performance_metrics[order(models_performance_metrics$Accuracy, decreasing = TRUE),]
final_metrics
glm_matrix
radial_matrix
glm_model <- train(Choice ~ ., data = train, method = "glm", trControl = control)
pred_logit = predict_prob(all_models$glm)
pred_svm.lin = predict_prob(all_models$svmLinear)
pred_svm.rbf = predict_prob(all_models$svmRadial)
pred_svm.poly = predict_prob(all_models$svmPoly)
calc_auc = function(prob) {
roc(test$Choice, prob)
}
auc_logit = calc_auc(pred_logit[,2])
auc_logit$auc[1] # we want this part included not the text so we index
auc_svm.lin = calc_auc(pred_svm.lin[,2])
auc_svm.rbf = calc_auc(pred_svm.rbf[,2])
auc_svm.poly = calc_auc(pred_svm.poly[,2])
# create a data frame of AUC results for each model
df_auc = bind_rows(data_frame(TPR = auc_logit$sensitivities,
FPR = 1 - auc_logit$specificities,
AUC = auc_logit$auc[1],
Model = "Logistic"),
data_frame(TPR = auc_svm.lin$sensitivities,
FPR = 1 - auc_svm.lin$specificities,
AUC = auc_svm.lin$auc[1],
Model = "Linear SVM"),
data_frame(TPR = auc_svm.rbf$sensitivities,
FPR = 1 - auc_svm.rbf$specificities,
AUC = auc_svm.rbf$auc[1],
Model = "RBF SVM"),
data_frame(TPR = auc_svm.poly$sensitivities,
FPR = 1 - auc_svm.poly$specificities,
AUC = auc_svm.poly$auc[1],
Model = "Poly SVM"))
df_auc %>%
ggplot(aes(FPR, TPR, color=Model)) +
geom_line(size = .5) +
theme_bw() +
coord_equal() +
# dashed line indicates a model that is classifying poorly
geom_abline(intercept = 0, slope = 1, color='gray37', size=1, linetype="dashed") +
labs(x = "FPR (1 - Specificity)",
y = "TPR (Sensitivity)",
title = "ROC Curve and AUC: Model Comparison")
#glm_model$finalModel$coefficients
#vif(glm_model$finalModel)
# Step 5: Identify numeric columns for creating histograms
# Automatically identify numeric columns
numeric_columns <- sapply(concat_data, is.numeric)
# Step 6: Create histograms for numeric variables
# List to store plots
plots <- list()
# Loop through numeric columns to create histograms and add them to the list
for (colname in names(concat_data)[numeric_columns]) {
p <- ggplot(train_data, aes_string(x = colname)) +
geom_histogram(bins = 30, fill = "blue", color = "black") +
ggtitle(paste("Histogram of", colname)) +
theme_minimal()
plots[[colname]] <- p  # Store each plot in the list
}
# Step 7: Arrange the histograms in a grid layout
do.call("grid.arrange", c(plots, ncol = 2))  # Adjust ncol for the number of
models_performance_metrics <- rbind(lm,glm,radial,polys,linearsvm,sigmoids)
final_metrics <- models_performance_metrics[order(models_performance_metrics$Accuracy, decreasing = TRUE),]
final_metrics
models_performance_metrics <- rbind(lm,glm,radial,polys,linearsvm,sigmoids)
final_metrics <- models_performance_metrics[order(models_performance_metrics$Accuracy, decreasing = TRUE),]
write.csv(final_metrics,"metrics.csv")
# Predict on the test set
glm_model <- train(Choice ~ ., data = train, method = "glm", trControl = control)
glm_predictions <- predict(glm_model, test, type = "prob")
glm_prob <- glm_predictions[,2]
glm_class <- ifelse(glm_prob > 0.5, 1, 0)  # Convert to binary classes (0 or 1)
glm_class <- as.factor(glm_class)
levels(glm_class) <- levels(test$Choice)
# Evaluate the mode
confusionMatrix(test$Choice,glm_class, positive = "Purchase")
roc_glm <- roc(as.numeric(test$Choice), glm_prob)
auc_glm <- auc(roc_glm)
#cat("Linear Regression AUC: ", auc_glm, "\n")
odds_ratio <- exp(coef(glm_model$finalModel))
print(odds_ratio)
varimport <- varImp(glm_model)
importance_df <- as.data.frame(varimport$importance)
importance_df$Variable <- rownames(importance_df)
# Create a plot using ggplot2
ggplot(importance_df, aes(x = reorder(Variable, Overall), y = Overall)) +
geom_bar(stat = "identity", fill = "steelblue") +
coord_flip() +
ggtitle("Variable Importance - Logistic Regression Model") +
xlab("Variables") +
ylab("Importance Score") +
theme_minimal()
summary(glm_model$finalModel)
vif(glm_model$finalModel)
varimport <- varImp(glm_model)
importance_df <- as.data.frame(varimport$importance)
importance_df$Variable <- rownames(importance_df)
# Create a plot using ggplot2
ggplot(importance_df, aes(x = reorder(Variable, Overall), y = Overall)) +
geom_bar(stat = "identity", fill = "steelblue") +
coord_flip() +
ggtitle("Variable Importance - Logistic Regression Model") +
xlab("Variables") +
ylab("Importance Score") +
theme_minimal()
summary(glm_model$finalModel)
H <- data.frame(vif(glm_model$finalModel))
varimport <- varImp(glm_model)
importance_df <- as.data.frame(varimport$importance)
importance_df$Variable <- rownames(importance_df)
# Create a plot using ggplot2
ggplot(importance_df, aes(x = reorder(Variable, Overall), y = Overall)) +
geom_bar(stat = "identity", fill = "steelblue") +
coord_flip() +
ggtitle("Variable Importance - Logistic Regression Model") +
xlab("Variables") +
ylab("Importance Score") +
theme_minimal()
summary(glm_model$finalModel)
H <- data.frame(vif(glm_model$finalModel))
H
par(mfrow=c(2,2))
plot(glm_model, which=c(1:4))
par(mfrow=c(2,2))
plot(glm_model$finalModel, which=c(1:4))
#plot(glm_model$finalModel, which = 1)
# Check deviance and degrees of freedom
# Running Box-Tidwell test with `boxTidwell`
all_models$svmRadial$bestTune
extractMetric(all_models$svmRadial)
#form1 = Choice ~.
set.seed(1)
form1 = Choice ~ .
radial_tune = tune.svm(form1, data=train_resample, gamma = seq(0.1, .1, by= 0.01), cost = seq(.1,1, by = .1))
radial_svm = svm(formula = form1, data = train_resample, gamma = radial_tune$best.parameters$gamma, cost = radial_tune$best.parameters$cost, decision.values=TRUE, probability=TRUE, kernel = "radial")
#summary(radial_svm)
radial_tune$best.parameters
print(radial_svm)
#form1 = Choice ~.
set.seed(1)
form1 = Choice ~ .
radial_tune = tune.svm(form1, data=train_resample, gamma = seq(0.1, .1, by= 0.01), cost = seq(.1,1, by = .1))
radial_svm = svm(formula = form1, data = train_resample, gamma = radial_tune$best.parameters$gamma, cost = radial_tune$best.parameters$cost, decision.values=TRUE, probability=TRUE, kernel = "radial")
#summary(radial_svm)
radial_tune$best.parameters
print(radial_svm$decision.values)
#form1 = Choice ~.
set.seed(1)
form1 = Choice ~ .
radial_tune = tune.svm(form1, data=train_resample, gamma = seq(0.1, .1, by= 0.01), cost = seq(.1,1, by = .1))
radial_svm = svm(formula = form1, data = train_resample, gamma = radial_tune$best.parameters$gamma, cost = radial_tune$best.parameters$cost, decision.values=TRUE, probability=TRUE, kernel = "radial")
summary(radial_svm)
radial_tune$best.parameters
#print(radial_svm$decision.values)
radial_predictions = predict(radial_svm, test, type="response", probability = TRUE, decision.values = TRUE)
caret::confusionMatrix(test$Choice, as.factor(radial_predictions), positive = "Purchase")
#svm_class <- ifelse(svmpredict > 0.5, 1, 0)  # Convert to binary classes (0 or 1)
#svm_class <- as.factor(svm_class)
#levels(svm_class) <- levels(test$Choice)
# Evaluate the model
#caret::confusionMatrix(as.factor(svmpredict), as.factor(test$Choice), positive = "1")
poly_tune = tune.svm(form1, data=train_resample, gamma = seq(0.1, .1, by= 0.01), cost = seq(.1,1, by = .1))
poly_svm = svm(formula = form1, data = train_resample, gamma = poly_tune$best.parameters$gamma, cost = poly_tune$best.parameters$cost, decision.values=TRUE, probability=TRUE, kernel = "poly")
summary(poly_svm)
poly_tune$best.parameters
models_performance_metrics <- rbind(lm,glm,radial,polys,linearsvm,sigmoids)
final_metrics <- models_performance_metrics[order(models_performance_metrics$Accuracy, decreasing = TRUE),]
final_metrics
glm_model$bestTune
# Step 4: Train a Generalized Linear Model (GLM) for classification (logistic regression)
set.seed(123)
control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
lm_model <- train(as.numeric(Choice) ~ ., data = train_resample, method = "lm", trControl = control)
# Step 6: Make predictions on the test set
lm_predictions <- predict(lm_model, test)
# Step 7: Convert continuous predictions to bi nary (0 or 1) using a threshold (e.g., 0.5)
lm_class <- ifelse(lm_predictions > 0.5, 1, 0)
lm_class <- as.factor(lm_class)
levels(lm_class) <- levels(test$Choice)
# Step 9: Calculate the confusion matrix
conf_matrix <- confusionMatrix(lm_class, as.factor(test$Choice), positive = "Purchase")
summary(lm_model)
conf_matrix
# Step 4: Train a Generalized Linear Model (GLM) for classification (logistic regression)
set.seed(123)
control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
lm_model <- train(as.numeric(Choice) ~ ., data = train_resample, method = "lm", trControl = control)
# Step 6: Make predictions on the test set
lm_predictions <- predict(lm_model, test)
# Step 7: Convert continuous predictions to bi nary (0 or 1) using a threshold (e.g., 0.5)
lm_class <- ifelse(lm_predictions > 0.5, 1, 0)
lm_class <- as.factor(lm_class)
levels(lm_class) <- levels(test$Choice)
# Step 9: Calculate the confusion matrix
conf_matrix <- confusionMatrix(lm_class, as.factor(test$Choice), positive = "Purchase")
summary(lm_model)
conf_matrix
lm_model$bestTune
