sigmoid_matrix <- confusionMatrix(as.factor(test$Choice),as.factor(sigmoid_predictions), positive = "Purchase")
sigmoid_accuracy <- sigmoid_matrix$overall[1]
sigmoid_accuracy
linear_matrix <- confusionMatrix(as.factor(test$Choice),as.factor(linear_predictions), positive = "Purchase")
linear_accuracy <- linear_matrix$overall[1]
linear_accuracy
evaluation_metrics <- function(model_auc, model_matrix, model_name) {
# Extract values from the confusion matrix for test data
TN_test <- model_matrix$table[1,1]  # True Negatives
FP_test <- model_matrix$table[1,2]  # False Positives
FN_test <- model_matrix$table[2,1]  # False Negatives
TP_test <- model_matrix$table[2,2]  # True Positives
# Calculate accuracy for the test data
accuracy_test <- (TP_test + TN_test) / sum(model_matrix$table)
# Calculate precision, recall, and F1 score for the test data
precision_test <- ifelse((TP_test + FP_test) > 0, TP_test / (TP_test + FP_test), 0)
recall_test <- ifelse((TP_test + FN_test) > 0, TP_test / (TP_test + FN_test), 0)
f1_score_test <- ifelse((precision_test + recall_test) > 0,
2 * ((precision_test * recall_test) / (precision_test + recall_test)),
0)
metric <- c("Model", "Accuracy", "Precision", "Recall", "F1_Score")
#metric <- c("Accuracy", "Precision", "Recall", "F1_Score")
metric_values <- c(model_name, accuracy_test, precision_test, recall_test, f1_score_test)
model_performance <-data.frame(
Model=model_name,
AUC=model_auc,
Accuracy=accuracy_test,
Precision=precision_test,
Recall=recall_test,
F1_score=f1_score_test)
return (model_performance)
# Print the performance metrics for the test data
#print(paste("Accuracy (Test):", accuracy_test))
#print(paste("Precision (Test):", precision_test))
#print(paste("Recall (Test):", recall_test))
#print(paste("F1 Score (Test):", f1_score_test))
}
glm <- evaluation_metrics(auc_glm, glm_matrix, "Logistic Regression")
lm <- evaluation_metrics(auc_lm, lm_matrix, "Linear Regression")
radial <- evaluation_metrics(auc_radial, radial_matrix, "RBF SVM")
polys <- evaluation_metrics(auc_poly, poly_matrix, "Polynomial SVM")
linearsvm <- evaluation_metrics(auc_linear, linear_matrix, "Linear SVM")
sigmoids <- evaluation_metrics(auc_sigmoid, sigmoid_matrix, "Sigmoid SVM")
models_performance_metrics <- rbind(lm,glm,radial,polys,linearsvm,sigmoids)
final_metrics <- models_performance_metrics[order(models_performance_metrics$Accuracy, decreasing = TRUE),]
final_metrics
glm_matrix
radial_matrix
glm_model <- train(Choice ~ ., data = train, method = "glm", trControl = control)
pred_logit = predict_prob(all_models$glm)
pred_svm.lin = predict_prob(all_models$svmLinear)
pred_svm.rbf = predict_prob(all_models$svmRadial)
pred_svm.poly = predict_prob(all_models$svmPoly)
calc_auc = function(prob) {
roc(test$Choice, prob)
}
auc_logit = calc_auc(pred_logit[,2])
auc_logit$auc[1] # we want this part included not the text so we index
auc_svm.lin = calc_auc(pred_svm.lin[,2])
auc_svm.rbf = calc_auc(pred_svm.rbf[,2])
auc_svm.poly = calc_auc(pred_svm.poly[,2])
# create a data frame of AUC results for each model
df_auc = bind_rows(data_frame(TPR = auc_logit$sensitivities,
FPR = 1 - auc_logit$specificities,
AUC = auc_logit$auc[1],
Model = "Logistic"),
data_frame(TPR = auc_svm.lin$sensitivities,
FPR = 1 - auc_svm.lin$specificities,
AUC = auc_svm.lin$auc[1],
Model = "Linear SVM"),
data_frame(TPR = auc_svm.rbf$sensitivities,
FPR = 1 - auc_svm.rbf$specificities,
AUC = auc_svm.rbf$auc[1],
Model = "RBF SVM"),
data_frame(TPR = auc_svm.poly$sensitivities,
FPR = 1 - auc_svm.poly$specificities,
AUC = auc_svm.poly$auc[1],
Model = "Poly SVM"))
df_auc %>%
ggplot(aes(FPR, TPR, color=Model)) +
geom_line(size = .5) +
theme_bw() +
coord_equal() +
# dashed line indicates a model that is classifying poorly
geom_abline(intercept = 0, slope = 1, color='gray37', size=1, linetype="dashed") +
labs(x = "FPR (1 - Specificity)",
y = "TPR (Sensitivity)",
title = "ROC Curve and AUC: Model Comparison")
#glm_model$finalModel$coefficients
#vif(glm_model$finalModel)
# Step 5: Identify numeric columns for creating histograms
# Automatically identify numeric columns
numeric_columns <- sapply(concat_data, is.numeric)
# Step 6: Create histograms for numeric variables
# List to store plots
plots <- list()
# Loop through numeric columns to create histograms and add them to the list
for (colname in names(concat_data)[numeric_columns]) {
p <- ggplot(train_data, aes_string(x = colname)) +
geom_histogram(bins = 30, fill = "blue", color = "black") +
ggtitle(paste("Histogram of", colname)) +
theme_minimal()
plots[[colname]] <- p  # Store each plot in the list
}
# Step 7: Arrange the histograms in a grid layout
do.call("grid.arrange", c(plots, ncol = 2))  # Adjust ncol for the number of
models_performance_metrics <- rbind(lm,glm,radial,polys,linearsvm,sigmoids)
final_metrics <- models_performance_metrics[order(models_performance_metrics$Accuracy, decreasing = TRUE),]
final_metrics
models_performance_metrics <- rbind(lm,glm,radial,polys,linearsvm,sigmoids)
final_metrics <- models_performance_metrics[order(models_performance_metrics$Accuracy, decreasing = TRUE),]
write.csv(final_metrics,"metrics.csv")
# Predict on the test set
glm_model <- train(Choice ~ ., data = train, method = "glm", trControl = control)
glm_predictions <- predict(glm_model, test, type = "prob")
glm_prob <- glm_predictions[,2]
glm_class <- ifelse(glm_prob > 0.5, 1, 0)  # Convert to binary classes (0 or 1)
glm_class <- as.factor(glm_class)
levels(glm_class) <- levels(test$Choice)
# Evaluate the mode
confusionMatrix(test$Choice,glm_class, positive = "Purchase")
roc_glm <- roc(as.numeric(test$Choice), glm_prob)
auc_glm <- auc(roc_glm)
#cat("Linear Regression AUC: ", auc_glm, "\n")
odds_ratio <- exp(coef(glm_model$finalModel))
print(odds_ratio)
varimport <- varImp(glm_model)
importance_df <- as.data.frame(varimport$importance)
importance_df$Variable <- rownames(importance_df)
# Create a plot using ggplot2
ggplot(importance_df, aes(x = reorder(Variable, Overall), y = Overall)) +
geom_bar(stat = "identity", fill = "steelblue") +
coord_flip() +
ggtitle("Variable Importance - Logistic Regression Model") +
xlab("Variables") +
ylab("Importance Score") +
theme_minimal()
summary(glm_model$finalModel)
vif(glm_model$finalModel)
varimport <- varImp(glm_model)
importance_df <- as.data.frame(varimport$importance)
importance_df$Variable <- rownames(importance_df)
# Create a plot using ggplot2
ggplot(importance_df, aes(x = reorder(Variable, Overall), y = Overall)) +
geom_bar(stat = "identity", fill = "steelblue") +
coord_flip() +
ggtitle("Variable Importance - Logistic Regression Model") +
xlab("Variables") +
ylab("Importance Score") +
theme_minimal()
summary(glm_model$finalModel)
H <- data.frame(vif(glm_model$finalModel))
varimport <- varImp(glm_model)
importance_df <- as.data.frame(varimport$importance)
importance_df$Variable <- rownames(importance_df)
# Create a plot using ggplot2
ggplot(importance_df, aes(x = reorder(Variable, Overall), y = Overall)) +
geom_bar(stat = "identity", fill = "steelblue") +
coord_flip() +
ggtitle("Variable Importance - Logistic Regression Model") +
xlab("Variables") +
ylab("Importance Score") +
theme_minimal()
summary(glm_model$finalModel)
H <- data.frame(vif(glm_model$finalModel))
H
par(mfrow=c(2,2))
plot(glm_model, which=c(1:4))
par(mfrow=c(2,2))
plot(glm_model$finalModel, which=c(1:4))
#plot(glm_model$finalModel, which = 1)
# Check deviance and degrees of freedom
# Running Box-Tidwell test with `boxTidwell`
all_models$svmRadial$bestTune
extractMetric(all_models$svmRadial)
#form1 = Choice ~.
set.seed(1)
form1 = Choice ~ .
radial_tune = tune.svm(form1, data=train_resample, gamma = seq(0.1, .1, by= 0.01), cost = seq(.1,1, by = .1))
radial_svm = svm(formula = form1, data = train_resample, gamma = radial_tune$best.parameters$gamma, cost = radial_tune$best.parameters$cost, decision.values=TRUE, probability=TRUE, kernel = "radial")
#summary(radial_svm)
radial_tune$best.parameters
print(radial_svm)
#form1 = Choice ~.
set.seed(1)
form1 = Choice ~ .
radial_tune = tune.svm(form1, data=train_resample, gamma = seq(0.1, .1, by= 0.01), cost = seq(.1,1, by = .1))
radial_svm = svm(formula = form1, data = train_resample, gamma = radial_tune$best.parameters$gamma, cost = radial_tune$best.parameters$cost, decision.values=TRUE, probability=TRUE, kernel = "radial")
#summary(radial_svm)
radial_tune$best.parameters
print(radial_svm$decision.values)
#form1 = Choice ~.
set.seed(1)
form1 = Choice ~ .
radial_tune = tune.svm(form1, data=train_resample, gamma = seq(0.1, .1, by= 0.01), cost = seq(.1,1, by = .1))
radial_svm = svm(formula = form1, data = train_resample, gamma = radial_tune$best.parameters$gamma, cost = radial_tune$best.parameters$cost, decision.values=TRUE, probability=TRUE, kernel = "radial")
summary(radial_svm)
radial_tune$best.parameters
#print(radial_svm$decision.values)
radial_predictions = predict(radial_svm, test, type="response", probability = TRUE, decision.values = TRUE)
caret::confusionMatrix(test$Choice, as.factor(radial_predictions), positive = "Purchase")
#svm_class <- ifelse(svmpredict > 0.5, 1, 0)  # Convert to binary classes (0 or 1)
#svm_class <- as.factor(svm_class)
#levels(svm_class) <- levels(test$Choice)
# Evaluate the model
#caret::confusionMatrix(as.factor(svmpredict), as.factor(test$Choice), positive = "1")
poly_tune = tune.svm(form1, data=train_resample, gamma = seq(0.1, .1, by= 0.01), cost = seq(.1,1, by = .1))
poly_svm = svm(formula = form1, data = train_resample, gamma = poly_tune$best.parameters$gamma, cost = poly_tune$best.parameters$cost, decision.values=TRUE, probability=TRUE, kernel = "poly")
summary(poly_svm)
poly_tune$best.parameters
models_performance_metrics <- rbind(lm,glm,radial,polys,linearsvm,sigmoids)
final_metrics <- models_performance_metrics[order(models_performance_metrics$Accuracy, decreasing = TRUE),]
final_metrics
glm_model$bestTune
# Step 4: Train a Generalized Linear Model (GLM) for classification (logistic regression)
set.seed(123)
control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
lm_model <- train(as.numeric(Choice) ~ ., data = train_resample, method = "lm", trControl = control)
# Step 6: Make predictions on the test set
lm_predictions <- predict(lm_model, test)
# Step 7: Convert continuous predictions to bi nary (0 or 1) using a threshold (e.g., 0.5)
lm_class <- ifelse(lm_predictions > 0.5, 1, 0)
lm_class <- as.factor(lm_class)
levels(lm_class) <- levels(test$Choice)
# Step 9: Calculate the confusion matrix
conf_matrix <- confusionMatrix(lm_class, as.factor(test$Choice), positive = "Purchase")
summary(lm_model)
conf_matrix
# Step 4: Train a Generalized Linear Model (GLM) for classification (logistic regression)
set.seed(123)
control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
lm_model <- train(as.numeric(Choice) ~ ., data = train_resample, method = "lm", trControl = control)
# Step 6: Make predictions on the test set
lm_predictions <- predict(lm_model, test)
# Step 7: Convert continuous predictions to bi nary (0 or 1) using a threshold (e.g., 0.5)
lm_class <- ifelse(lm_predictions > 0.5, 1, 0)
lm_class <- as.factor(lm_class)
levels(lm_class) <- levels(test$Choice)
# Step 9: Calculate the confusion matrix
conf_matrix <- confusionMatrix(lm_class, as.factor(test$Choice), positive = "Purchase")
summary(lm_model)
conf_matrix
lm_model$bestTune
library(tidyverse)
library(here)
library(ggplot2)
library(gridExtra)
library(tseries)
install.packages('teries')
library(tidyverse)
library(here)
library(ggplot2)
library(gridExtra)
library(tseries)
install.packages('installr')
updateR()
library(installr)
updateR()
r
R
r --verison
r --version
r -version
version
version
library(tidyverse)
install.packages('tidyverse')
install.packages('here')
library(tidyverse)
library(here)
library(ggplot2)
library(gridExtra)
install.packages('gridExtra')
install.packages('tseries')
install.packages('quantmod')
library(tidyverse)
library(here)
library(ggplot2)
library(gridExtra)
library(tseries)
library(dplyr)
library(quantmod)
here()
# Use the here function to construct the file path and import the dataset
data <- read.csv(here("dow_jones_index.data.csv"), header = TRUE, sep = ",")
library(tidyverse)
library(here)
library(ggplot2)
library(gridExtra)
library(tseries)
library(dplyr)
library(quantmod)
here()
pwd
cwd
getwd()
# Use the here function to construct the file path and import the dataset
data <- read.csv(here("/da-6813-901-data-analytics-applications/case3/dow_jones_index.data.csv"), header = TRUE, sep = ",")
library(tidyverse)
library(here)
library(ggplot2)
library(gridExtra)
library(tseries)
library(dplyr)
library(quantmod)
here()
setwd("/Users/c2cypher/codebase/msda/msda-grad-school/da-6813-901-data-analytics-applications/case3")
# Use the here function to construct the file path and import the dataset
data <- read.csv(here("dow_jones_index.data.csv"), header = TRUE, sep = ",")
library(tidyverse)
library(here)
library(ggplot2)
library(gridExtra)
library(tseries)
library(dplyr)
library(quantmod)
here()
# Use the here function to construct the file path and import the dataset
> setwd("/Users/c2cypher/codebase/msda/msda-grad-school/da-6813-901-data-analytics-applications/case3")
# Use the here function to construct the file path and import the dataset
setwd("/Users/c2cypher/codebase/msda/msda-grad-school/da-6813-901-data-analytics-applications/case3")
data <- read.csv(here("dow_jones_index.data.csv"), header = TRUE, sep = ",")
# Use the here function to construct the file path and import the dataset
setwd("/Users/c2cypher/codebase/msda/msda-grad-school/da-6813-901-data-analytics-applications/case3")
data <- read.csv("dow_jones_index.data.csv", header = TRUE, sep = ",")
# Use the here function to construct the file path and import the dataset
setwd("/Users/c2cypher/codebase/msda/msda-grad-school/da-6813-901-data-analytics-applications/case3")
data <- read.csv("dow_jones_index.data.csv", header = TRUE, sep = ",")
library(tidyverse)
library(here)
library(ggplot2)
library(gridExtra)
library(tseries)
library(dplyr)
library(quantmod)
setwd("/Users/c2cypher/codebase/msda/msda-grad-school/da-6813-901-data-analytics-applications/case3")
# Use the here function to construct the file path and import the dataset
data <- read.csv("dow_jones_index.data.csv", header = TRUE, sep = ",")
library(tidyverse)
library(here)
library(ggplot2)
library(gridExtra)
library(tseries)
library(dplyr)
library(quantmod)
setwd("/Users/c2cypher/codebase/msda/msda-grad-school/da-6813-901-data-analytics-applications/case3")
# Use the here function to construct the file path and import the dataset
data <- read.csv("dow_jones_index.data.csv", header = TRUE, sep = ",")
head(data)
str(data)
# Check for missing data or NA values in the dataset
missing_data_summary <- colSums(is.na(data))
missing_data_summary
# removed.
data_clean <- data
# Convert 'quarter' variable to a factor
data_clean$quarter <- as.factor(data_clean$quarter)
# Convert 'stock' variable to a factor
data_clean$stock <- as.factor(data_clean$stock)
# Convert 'date' variable to Date class format
data_clean$date <- as.Date(data_clean$date, format = "%m/%d/%Y")
# Remove non-numeric characters from 'open', 'high', 'low', and 'close' before converting to numeric
data_clean$open <- as.numeric(gsub("[^0-9.]", "", data_clean$open))
data_clean$high <- as.numeric(gsub("[^0-9.]", "", data_clean$high))
data_clean$low <- as.numeric(gsub("[^0-9.]", "", data_clean$low))
data_clean$close <- as.numeric(gsub("[^0-9.]", "", data_clean$close))
# Convert 'volume' variable to numeric
data_clean$volume <- as.numeric(gsub("[^0-9.]", "", data_clean$volume))
# # Convert 'percent_change_price' to numeric by removing '%' and dividing by 100
# data_clean$percent_change_price <- as.numeric(gsub("%", "", data_clean$percent_change_price)) / 100
#
# Convert 'percent_change_volume_over_last_wk' to numeric by removing '%'
data_clean$percent_change_volume_over_last_wk <- as.numeric(gsub("%", "", data_clean$percent_change_volume_over_last_wk))
# Remove non-numeric characters from 'previous_weeks_volume', 'next_weeks_open', and 'next_weeks_close' before converting to numeric
data_clean$previous_weeks_volume <- as.numeric(gsub("[^0-9.]", "", data_clean$previous_weeks_volume))
data_clean$next_weeks_open <- as.numeric(gsub("[^0-9.]", "", data_clean$next_weeks_open))
data_clean$next_weeks_close <- as.numeric(gsub("[^0-9.]", "", data_clean$next_weeks_close))
# # Convert 'percent_change_next_weeks_price' to numeric by removing '%' and dividing by 100
# data_clean$percent_change_next_weeks_price <- as.numeric(gsub("%", "", data_clean$percent_change_next_weeks_price)) / 100
# Convert 'days_to_next_dividend' to numeric
data_clean$days_to_next_dividend <- as.numeric(gsub("[^0-9.]", "", data_clean$days_to_next_dividend))
str(data_clean)
# Convert 'quarter' variable to a factor
data_clean$quarter <- as.factor(data_clean$quarter)
# Convert 'stock' variable to a factor
data_clean$stock <- as.factor(data_clean$stock)
# Convert 'date' variable to Date class format
data_clean$date <- as.Date(data_clean$date, format = "%m/%d/%Y")
# Remove non-numeric characters from 'open', 'high', 'low', and 'close' before converting to numeric
data_clean$open <- as.numeric(gsub("[^0-9.]", "", data_clean$open))
data_clean$high <- as.numeric(gsub("[^0-9.]", "", data_clean$high))
data_clean$low <- as.numeric(gsub("[^0-9.]", "", data_clean$low))
data_clean$close <- as.numeric(gsub("[^0-9.]", "", data_clean$close))
# Convert 'volume' variable to numeric
data_clean$volume <- as.numeric(gsub("[^0-9.]", "", data_clean$volume))
# # Convert 'percent_change_price' to numeric by removing '%' and dividing by 100
# data_clean$percent_change_price <- as.numeric(gsub("%", "", data_clean$percent_change_price)) / 100
#
# Convert 'percent_change_volume_over_last_wk' to numeric by removing '%'
data_clean$percent_change_volume_over_last_wk <- as.numeric(gsub("%", "", data_clean$percent_change_volume_over_last_wk))
# Remove non-numeric characters from 'previous_weeks_volume', 'next_weeks_open', and 'next_weeks_close' before converting to numeric
data_clean$previous_weeks_volume <- as.numeric(gsub("[^0-9.]", "", data_clean$previous_weeks_volume))
data_clean$next_weeks_open <- as.numeric(gsub("[^0-9.]", "", data_clean$next_weeks_open))
data_clean$next_weeks_close <- as.numeric(gsub("[^0-9.]", "", data_clean$next_weeks_close))
# # Convert 'percent_change_next_weeks_price' to numeric by removing '%' and dividing by 100
# data_clean$percent_change_next_weeks_price <- as.numeric(gsub("%", "", data_clean$percent_change_next_weeks_price)) / 100
# Convert 'days_to_next_dividend' to numeric
data_clean$days_to_next_dividend <- as.numeric(gsub("[^0-9.]", "", data_clean$days_to_next_dividend))
str(data_clean)
# Step 1: Inspect the unique stock symbols
unique_stocks <- unique(data_clean$stock) # Adjust column name if different
# Step 2: Randomly select 5 unique stocks
set.seed(123) # Set seed for reproducibility
selected_stocks <- sample(unique_stocks, 30)
# Step 3: Create a directory for each stock and save its data
for (stock in selected_stocks) {
# Filter data for the current stock
stock_data <- data_clean %>%
filter(stock == !!stock) %>%
select(stock,date, open, high, low, close, volume) # Adjust column names to match your dataset
# Create file path for each stock
file_name <- here( paste0("stock_", stock, ".csv"))
# Save the stock data as a CSV file within its folder
file_name <- here( paste0("stock_", stock, ".csv"))
write.csv(stock_data, file_name, row.names = FALSE)
}
# Step 1: Extract unique stock names from the main dataset
stock_symbols <- unique(data_clean$stock) # Ensure 'data_clean' is already processed
# Step 2: Load data for each stock into individual variables
for (symbol in stock_symbols) {
# Construct file name dynamically
file_path <- here(paste0("stock_", symbol, ".csv"))
# Check if the file exists
if (file.exists(file_path)) {
# Read the stock data
assign(symbol, read.csv(file_path, header = TRUE, sep = ","))
cat("Loaded data for stock:", symbol, "\n")
} else {
warning(paste("File not found for stock:", symbol))
}
}
# Example: Check the structure of one dataset (e.g., AAPL)
if (exists("AAPL")) {
str(AAPL)
}
# Step 2: Select only the relevant variables
clean_stock <- data_clean %>%
select(stock,date, open, high, low, close, volume) # Adjust column names to match your dataset
# Step 3: Save the cleaned dataset as dowjones.csv
output_path <- here("clean_stock.csv")
write.csv(data_clean, output_path, row.names = FALSE)
clean_stock <- read.csv(here("clean_stock.csv"), header = TRUE, sep = ",")
str(clean_stock)
str(GE)
str(clean_stock)
# Convert 'date' column to Date format and numeric conversion for the other columns
clean_stock <- data %>%
mutate(
date = as.Date(date, format = "%m/%d/%Y"), # Convert 'date' to Date format
open = as.numeric(gsub("\\$", "", open)),  # Remove dollar sign and convert to numeric
high = as.numeric(gsub("\\$", "", high)),  # Remove dollar sign and convert to numeric
low = as.numeric(gsub("\\$", "", low)),    # Remove dollar sign and convert to numeric
close = as.numeric(gsub("\\$", "", close)),# Remove dollar sign and convert to numeric
volume = as.numeric(volume)                # Ensure 'volume' is numeric
)
# Remove the 'stock' column
clean_stock <- clean_stock %>%
select(date, open, high, low, close, volume)
str(clean_stock)
library(dplyr)
# Ensure columns are properly formatted
clean_stock <- clean_stock %>%
mutate(
date = as.Date(date, format = "%m/%d/%Y"),
open = as.numeric(gsub("\\$", "", open)),
high = as.numeric(gsub("\\$", "", high)),
low = as.numeric(gsub("\\$", "", low)),
close = as.numeric(gsub("\\$", "", close)),
volume = as.numeric(volume)
)
# Check for issues in data conversion
print(summary(data))  # Provides an overview of the data
# Group by 'date' and calculate averages, handling NAs
averaged_data <- clean_stock %>%
group_by(date) %>%
summarize(
avg_open = mean(open, na.rm = TRUE),
avg_high = mean(high, na.rm = TRUE),
avg_low = mean(low, na.rm = TRUE),
avg_close = mean(close, na.rm = TRUE),
avg_volume = mean(volume, na.rm = TRUE)
)
# View the cleaned data
print(head(averaged_data))
# Step 1: Define stock symbols
stock_symbols <- unique(data_clean$stock)
# Step 2: Remove the 'stock' column for each dataset dynamically
for (symbol in stock_symbols) {
# Check if the dataset exists in the environment
if (exists(symbol)) {
# Get the dataset
stock_data <- get(symbol)
# Remove the 'stock' column
stock_data <- stock_data %>% select(-stock)
# Save the modified dataset back to its original variable
assign(symbol, stock_data)
# Print confirmation
cat("Removed 'stock' column for:", symbol, "\n")
} else {
warning(paste("Dataset not found for stock:", symbol))
}
}
