{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaf686b-2b77-479b-b380-85b8be06c0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTES\n",
    "\n",
    "# MIDTERM\n",
    "\n",
    "## Will be posted on Tuesday.\n",
    "## \"MAIN\" topics will be everything from first Midterm until now\n",
    "## The exam is cumulitive. Expect at least 2 programming questions where you need to write code and 2 regex questions.'\n",
    "## 2 hours to complete. Same format as Exam 1\n",
    "## We will have Q/A next Tuesday.\n",
    "\n",
    "# Pipelines\n",
    "## https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "\n",
    "\n",
    "# Feature Transformation/Selection\n",
    "\n",
    "## MinMaxScaler\n",
    "\n",
    "## StandardScaler\n",
    "\n",
    "## Normalizer\n",
    "\n",
    "## SelectKBest\n",
    "\n",
    "## PCA\n",
    "\n",
    "# Missing Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07c9bba-8a41-4843-b873-51586e6fd3d8",
   "metadata": {},
   "source": [
    "# Exercise 1: Missing Data\n",
    "\n",
    "Access to safe drinking-water is essential to health, a basic human right and a component of effective policy for health protection. This is important as a health and development issue at a national, regional and local level. In some regions, it has been shown that investments in water supply and sanitation can yield a net economic benefit, since the reductions in adverse health effects and health care costs outweigh the costs of undertaking the interventions. Yet, water drinkability is measured by multiple factors, many of which can not be used alone. Hence, this exercise we will develop methods of predicting whether water is drinkable based on variuos measured characteristics.\n",
    "\n",
    "**YOUR TASK:** There is missing data in the csv file. For each column, calculate how many missing values, i.e., count the number of ```np.nan``` values.\n",
    "\n",
    "**TIME**: 10 Minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73d1dea7-cead-41fa-880f-256a207a5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "X_dict = []\n",
    "y = []\n",
    "with open('water_potability_missing.csv') as iFile:\n",
    "    iCSV = csv.reader(iFile, delimiter=',')\n",
    "    header = next(iCSV)\n",
    "    for row in iCSV:\n",
    "        data = {}\n",
    "        for f, x in zip(header[:-1], row[:-1]):\n",
    "            if x != '':\n",
    "                data[f] = float(x)\n",
    "            else:\n",
    "                data[f] = np.nan\n",
    "        X_dict.append(data)\n",
    "        y.append(int(row[-1]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "165ce519-afa6-4c6a-b2e8-037e49c433a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ph': nan,\n",
       " 'Hardness': 204.8904554713363,\n",
       " 'Solids': 20791.318980747026,\n",
       " 'Chloramines': 7.300211873184757,\n",
       " 'Sulfate': 368.51644134980336,\n",
       " 'Conductivity': 564.3086541722439,\n",
       " 'Organic_carbon': 10.3797830780847,\n",
       " 'Trihalomethanes': 86.9909704615088,\n",
       " 'Turbidity': 2.9631353806316407}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e1ab9ba-1a44-4b7d-99ac-6c395fb21d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce12ac47-3a3a-467f-983c-5dc36db80db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "######################\n",
    "# Split dataset into a training and testing portion here.\n",
    "\n",
    "X_train_dict, X_test_dict, y_train, y_test = train_test_split(X_dict, y, test_size=0.2) # Replace the ... with your code to split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "843bb03d-03e6-433a-874d-16befc2a358a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2620 656\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b956315-3e6e-4b97-b48d-9d0cf7cd619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell creates the matrix from each list of dictionaries.  You do NOT need to edit this.\n",
    "vec = DictVectorizer(sparse=False)\n",
    "\n",
    "X_train = vec.fit_transform(X_train_dict)\n",
    "X_test = vec.transform(X_test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0d42615-1c85-4e5d-98cf-60fe0010da5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chloramines',\n",
       " 'Conductivity',\n",
       " 'Hardness',\n",
       " 'Organic_carbon',\n",
       " 'Solids',\n",
       " 'Sulfate',\n",
       " 'Trihalomethanes',\n",
       " 'Turbidity',\n",
       " 'ph']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3910c2fa-d0cf-445a-91a7-fc9c5ec392e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2620, 9)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b31f4fd5-ef0e-4266-860d-6ad6b6441dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 9)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE CODE TO ANSWER QUESTIONS HERE. HINT: play with np.isnan(MATRIX_HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70e8a4c-529c-4e50-b941-e5184f46bf05",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exercise 2: Model Selection\n",
    "\n",
    "The data has been split into a training and test dataset. Now, your task is to train a machine learning classifier on the dataset. Specifically, we are going to train the Random classifier in scikit-learn. Your task is to evaluate three methods of handling missing data scikit-learn's SimpleImputer: https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer\n",
    "\n",
    "1. mean\n",
    "2. median\n",
    "3. constant\n",
    "    1. Experiment with setting all missing values to zero.\n",
    "    \n",
    "Report the training F1 and cross-validation F1 in the following table:\n",
    "\n",
    "|Model|Impute Strategy| Training F1 | Validation F1|\n",
    "|----|----|----|----|\n",
    "|Random Forest| mean| SCORE HERE | SCORE HERE |\n",
    "Random Forest|median | SCORE HERE | SCORE HERE |\n",
    "Random Forest|constant| SCORE HERE | SCORE HERE |\n",
    "\n",
    "Finally, answer the questions: Is the training F1 higher then the validation F1? Why or why not?\n",
    "\n",
    "**TIME**: 10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debcff99-159a-484a-ab9b-75324a918928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "####################################################\n",
    "\n",
    "# Write code to use SimpleImputer here. (See documentation)\n",
    "\n",
    "imputer = SimpleImputer(...) # MODIFY THIS LINE\n",
    "\n",
    "X_train_impute = imputer.fit_transform(X_train)\n",
    "X_test_impute = imputer.transform(X_test)\n",
    "\n",
    "####################################################\n",
    "\n",
    "params = {'n_estimators': [50, 100, 250, 500]}\n",
    "\n",
    "base_clf = RandomForestClassifier()\n",
    "\n",
    "clf = GridSearchCV(base_clf, params, cv = 5, scoring='f1')\n",
    "\n",
    "clf.fit(X_train_impute, y_train)\n",
    "\n",
    "preds = clf.predict(X_train_impute)\n",
    "\n",
    "f1 = f1_score(y_train, preds)\n",
    "print(\"Best Parameters:\", clf.best_params_)\n",
    "print(f\"Training F1: {f1:.4f}\") # This is the \"Training F1\"\n",
    "print(f\"Validation F1: {clf.best_score_:.4f}\") # This is the Validation F1 from the cross-validation procedure in GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fb41d1-cb78-4580-b146-5f42588ce695",
   "metadata": {},
   "source": [
    "# Exercise 3: Feature transformations and More Model Selection\n",
    "\n",
    "We also want to experiment with the the SVC classifier and different feature transformaiton methods. For this task complete the following:\n",
    "\n",
    "1. Apply the StandardScaler and MinMaxScaler to the to X_train and X_test (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html, and https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n",
    "2. Apply the SimpleImputer() with a strategy where strategy is set to \"constant\" and the fill_value is set to 0.\n",
    "\n",
    "|Model|Impute Strategy | Normalization Method | Training F1 | Validation F1|\n",
    "|----|----|----|----|----|\n",
    "|SVC| mean| StandardScaler | SCORE HERE | SCORE HERE |\n",
    "|SVC|median |  StandardScaler | SCORE HERE | SCORE HERE |\n",
    "|SVC|constant|  StandardScaler | SCORE HERE | SCORE HERE |\n",
    "|SVC| mean| MinMaxScaler | SCORE HERE | SCORE HERE |\n",
    "|SVC|median |  MinMaxScaler | SCORE HERE | SCORE HERE |\n",
    "|SVC|constant|  MinMaxScaler | SCORE HERE | SCORE HERE |\n",
    "\n",
    "\n",
    "What combination gives the best validation results?\n",
    "\n",
    "**TIME**: 10 Minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adc99c3-ad6a-40cf-a66a-f05ac6a3ea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "####################################################\n",
    "\n",
    "# Write code to use SimpleImputer here. (See documentation)\n",
    "\n",
    "imputer = SimpleImputer(...) # MODIFY THIS LINE\n",
    "\n",
    "X_train_impute = imputer.fit_transform(X_train)\n",
    "X_test_impute = imputer.transform(X_test)\n",
    "\n",
    "####################################################\n",
    "\n",
    "####################################################\n",
    "\n",
    "# Write code to \"normalize\" or \"scale\" data here.\n",
    "\n",
    "scale =  # MODIFY THIS LINE (instantiate StandardScaler or MinMaxScaler here)\n",
    "\n",
    "X_train_impute_scale = scale.fit_transform(X_train_impute)\n",
    "X_test_impute_scale = scale.transform(X_test_impute)\n",
    "\n",
    "####################################################\n",
    "\n",
    "params = {'C': [.001, .01, .1, 1., 10., 100., 1000.]}\n",
    "\n",
    "base_clf = SVC()\n",
    "\n",
    "clf = GridSearchCV(base_clf, params, cv = 5, scoring='f1')\n",
    "\n",
    "clf.fit(X_train_impute_scale, y_train)\n",
    "\n",
    "preds = clf.predict(X_train_impute_scale)\n",
    "\n",
    "f1 = f1_score(y_train, preds)\n",
    "print(\"Best Parameters:\", clf.best_params_)\n",
    "print(f\"Training F1: {f1:.4f}\") # This is the \"Training F1\"\n",
    "print(f\"Validation F1: {clf.best_score_:.4f}\") # This is the Validation F1 from the cross-validation procedure in GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37655547-ccc7-439c-86ca-9b0f6ccd36ae",
   "metadata": {},
   "source": [
    "# Exercise 4: Model Assessment\n",
    "\n",
    "Take the best and hyperparameters found from the prior exercises, refit the model and evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9f8c64-014e-45bb-a926-7a004697dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c02ab98-a420-4ce7-a30a-0a7dff47e98d",
   "metadata": {},
   "source": [
    "# Extra Exercise: Plot the data in two dimensions\n",
    "\n",
    "For this exercise, you should apply PCA to the data to reduce to two dimensions to plot the data. Remeber to apply PCA, you need to 1) apply the StandardScalar and 2) apply PCA. Because our dataset has missing values, you will need to apply the SimpleImputer before applying the StandardScalar.\n",
    "\n",
    "Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278c9a0f-2209-4653-913e-324af6cecfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "####################################################\n",
    "# STEP 1\n",
    "\n",
    "# Write code to use SimpleImputer here. (See documentation)\n",
    "\n",
    "imputer = SimpleImputer(...) # MODIFY THIS LINE (use mean)\n",
    "\n",
    "X_train_impute = imputer.fit_transform(X_train)\n",
    "X_test_impute = imputer.transform(X_test)\n",
    "\n",
    "####################################################\n",
    "\n",
    "\n",
    "####################################################\n",
    "# STEP 2\n",
    "# Write code to use SimpleImputer here. (See documentation)\n",
    "\n",
    "scaler = # Instantiate StandardScalar Here\n",
    "\n",
    "X_train_impute_scale = scaler.fit_transform(X_train_impute)\n",
    "X_test_impute_scale = scaler.transform(X_test_impute)\n",
    "\n",
    "####################################################\n",
    "\n",
    "\n",
    "####################################################\n",
    "# STEP 3\n",
    "# Write code to use SimpleImputer here. (See documentation)\n",
    "\n",
    "pca = # Instantiate PCA Here (Make sure you set n_components = 2)\n",
    "\n",
    "X_train_impute_scale_pca = pca.fit_transform(X_train_impute_scale)\n",
    "X_test_impute_scale_pca = pca.transform(X_test_impute_scale)\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab56d0e-de8e-4f98-8205-f33e74e661dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# DO NOT Change this cell!!!\n",
    "\n",
    "y_train2 = np.array(y_train)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = [1, 0]\n",
    "colors = ['r', 'g']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = y_train2 == target\n",
    "    ax.scatter(X_train_impute_scale_pca[indicesToKeep,0]\n",
    "               , X_train_impute_scale_pca[indicesToKeep,1]\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afa659a-d5ce-4cce-9138-17056133ff36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
