##################################################
athletes=read.table("athletes.txt", header=TRUE)
names(athletes)[1]="Sex"
str(athletes)
###################################################
# log transformation
##################################################
athletes=read.table("athletes.txt", header=TRUE)
names(athletes)[1]="Sex"
###################################################
# log transformation
##################################################
athletes=read.table("athletes.txt", header=TRUE)
names(athletes)[1]="Sex"
str(athletes)
lm.athletes=lm(Ferr~.-Sport, data=athletes)
model.stepwise<-ols_step_both_p(lm.athletes, pent = 0.05, prem = 0.05, details = FALSE)
model.stepwise
lm.step=lm(Ferr~Sex+BMI+LBM, data=athletes)
summary(lm.step)
par(mfrow=c(2,2))
plot(lm.step, which=1:4)  # diagnostics plot
#####################################################
lm.log.athletes=lm(log(Ferr)~.-Sport, data=athletes)
model.log.stepwise<-ols_step_both_p(lm.log.athletes, pent = 0.05, prem = 0.05, details = FALSE)
model.log.stepwise
lm.log.step=lm(log(Ferr)~Sex+BMI+LBM, data=athletes)
summary(lm.log.step)
plot(lm.log.step, which=1:4)  # diagnostics plot
athletes=read.table("athletes.txt", header=TRUE)
names(athletes)[1]="Sex"
str(athletes)
lm.athletes=lm(Ferr~.-Sport, data=athletes)
model.stepwise<-ols_step_both_p(lm.athletes, pent = 0.05, prem = 0.05, details = FALSE)
model.stepwise
lm.step=lm(Ferr~Sex+BMI+LBM, data=athletes)
summary(lm.step)
par(mfrow=c(2,2))
plot(lm.step, which=1:4)  # diagnostics plot
#####################################################
lm.log.athletes=lm(log(Ferr)~.-Sport, data=athletes)
model.log.stepwise<-ols_step_both_p(lm.log.athletes, pent = 0.05, prem = 0.05, details = FALSE)
model.log.stepwise
lm.log.step=lm(log(Ferr)~Sex+BMI+LBM, data=athletes)
summary(lm.log.step)
plot(lm.log.step, which=1:4)  # diagnostics plot
View(lm.multicol3)
athletes=read.table("athletes.txt", header=TRUE)
athletes=read.table("athletes.txt")
names(athletes)[1]="Sex"
str(athletes)
lm.athletes=lm(Ferr~.-Sport, data=athletes)
athletes=read.table("athletes.txt", header=TRUE)
names(athletes)[1]="Sex"
str(athletes)
lm.athletes=lm(Ferr~.-Sport, data=athletes)
model.stepwise<-ols_step_both_p(lm.athletes, pent = 0.05, prem = 0.05, details = FALSE)
model.stepwise
lm.step=lm(Ferr~Sex+BMI+LBM, data=athletes)
summary(lm.step)
par(mfrow=c(2,2))
plot(lm.step, which=1:4)  # diagnostics plot
#####################################################
lm.log.athletes=lm(log(Ferr)~.-Sport, data=athletes)
model.log.stepwise<-ols_step_both_p(lm.log.athletes, pent = 0.05, prem = 0.05, details = FALSE)
model.log.stepwise
lm.log.step=lm(log(Ferr)~Sex+BMI+LBM, data=athletes)
summary(lm.log.step)
plot(lm.log.step, which=1:4)  # diagnostics plot
#########################################################
# regression model with categorical variable
########################################################
professor=read.csv("Professor.csv",header=TRUE)
str(professor)
is.factor(professor$Gender);
is.character(professor$Gender)
#############################################################
# # main effect model
##############################################################
#################################################################
# regressuion with factor/ character "Gender"
summary(lm(SALARY~Gender+PUBS, data=professor))
#################################################################
# regression with numerically coded Gender.num
Gender.num=ifelse(professor$Gender=="Female",0,1) # ref group = female
Gender.num2=ifelse(professor$Gender=="Female",1,0) # ref group = male
professor=data.frame(professor,Gender.num,Gender.num2)
View(professor)
summary(lm(SALARY~Gender.num+PUBS, data=professor))
summary(lm(SALARY~Gender.num2+PUBS, data=professor))
summary(lm(SALARY~Gender*PUBS, data=professor))  # main effect and interaction model
############################################################################
# regression model with categorical variable with more than three levels
############################################################################
tooth <- read.csv("ToothGrowth.csv") # data analyzed in balanced ANOVA
str(tooth)
tooth$Dose= as.factor(tooth$Dose)   # three levels: 0.5/ 1/ 2
str(tooth)
summary(lm(Toothlength ~ Dose, data=tooth))
######################################################################
# simulation study
#####################################################################
drinking=read.csv("drinking.csv", header=TRUE)
# simulation data generation - predictors with perfect or high correlation
set.seed(5678)
alcohol2= 3* drinking$alcohol   # alcohol & alcohol2 : perfect linear relationship
alcohol3= 3*drinking$alcohol+rnorm(15,0,0.5) # alcohol & alcohol3 : almost linear but not perfectly linear
# new dataframe with simulated predictors - alcohol2 & alcohol3
drinking.new=data.frame(cbind(drinking, alcohol2, alcohol3))
View(drinking.new)
# check correlation through pairwise scatter plot and correation
pairs(drinking.new[ ,c(2:5)])
cor(drinking.new$alcohol, drinking.new$alcohol2)
cor(drinking.new$alcohol, drinking.new$alcohol3)
plot(drinking.new$alcohol, drinking.new$alcohol2)
plot(drinking.new$alcohol, drinking.new$alcohol3)
# run linear regressions with multicollinearity issue
# perfect correlation
lm.multicol1 = lm(cirrhosis~alcohol+alcohol2, data=drinking.new)
# high correlation but not perfect
lm.multicol2 = lm(cirrhosis~alcohol+alcohol3, data=drinking.new)
summary(lm.multicol1)
summary(lm.multicol2)
# run simple linear regression
lm.multicol1 = lm(cirrhosis~alcohol, data=drinking.new)
summary(lm.multicol1)
lm.multicol2 = lm(cirrhosis~alcohol2, data=drinking.new)
summary(lm.multicol2)
lm.multicol3 = lm(cirrhosis~alcohol3, data=drinking.new)
summary(lm.multicol3)
lm.crime <- lm(y~., data=UScrime)   # run the model with all variables in data set
summary(lm.crime)
summary(lm.crime)$r.squared  # R-square: percentage of variation of y explained by the model
summary(lm.crime)$adj.r.squared  # adjusted R-square: with penalty for large (large p) model
# VIF to check multicollinearity
vif(lm.crime)   # in package "car"
lm.crime2=lm(y~.-Po2, data=UScrime)   # all predictors except Po2
vif(lm.crime2)
lm.crime3=lm(y~.-Po2-GDP, data=UScrime)   # all predictors except Po2 and GDP
vif(lm.crime3)
par(mfrow=c(2,2))
plot(lm.crime3, which=1:4)  # diagnostics plot
######################################################################
# model selection
#######################################################################
# 1. automatic selection (forward, backward, stepwise selection)
########################################################################
# stepwise selection
model.stepwise<-ols_step_both_p(lm.crime3, pent = 0.10, prem = 0.10, details = FALSE)
model.stepwise
plot(model.stepwise)
lm.step=lm(y~Po1+Ineq+Ed+M+Prob+U2, data=UScrime)
summary(lm.step)
# forward selection
model.forward<-ols_step_forward_p(lm.crime3, penter = 0.10, details = F) # penter: threshold p-value for enter
model.forward    # final model summary
plot(model.forward)
# backward selection
model.backward<-ols_step_backward_p(lm.crime3, prem = 0.10, details = F) # prem: threshold p-value for removal
model.backward
###############################################
# 2. best subset selection (AIC, SBC, adjusted R-squre, C(p) etc.)
model.best.subset<-ols_step_best_subset(lm.crime3) # takes some of time 5-10 min. Be patient :)
model.best.subset
library(DescTools); library(ResourceSelection)
setwd("/Users/c2cypher/codebase/msda/msda-grad-school/sta-6443-902-data_analytics_algorithms/datasets")
resp = read.csv("resp.csv", header=TRUE)
str(resp)
resp$Cent = as.factor(resp$Cent)
resp$BL = as.factor(resp$BL)
model.full = glm(V1 ~ .-V4, data=resp, family = binomial) # full model: all predictors
glm.resp <- step(model.full,
direction="backward",test="Chisq", trace = F, k=log(nrow(resp)))
summary(glm.resp)
exp(glm.resp$coefficients)
View(resp)
#| warning: false
library(tidyverse)
library(olsrr)
library(car)
library(ResourceSelection)
library(DescTools)
setwd("/Users/c2cypher/codebase/msda/msda-grad-school/sta-6443-902-data_analytics_algorithms/exam_2/final_exam")
df = read.csv('birthweight_final.csv')
df$Black = as.factor(df$Black)
df$Married = as.factor(df$Married)
df$Boy = as.factor(df$Boy)
df$MomSmoke = as.factor(df$MomSmoke)
df$Ed = as.factor(df$Ed)
str(df)
q1.lm = lm(Weight ~ Black + Married + Boy + MomSmoke + Ed + MomAge + MomWtGain +
Visit, data = df)
summary(q1.lm)
model.stepwise = ols_step_both_p(q1.lm, pent = 0.01, prem = 0.01, details = FALSE)
model.stepwise
model.lm.stepwise = lm(Weight ~ MomWtGain + MomSmoke + Black, data = df)
summary(model.lm.stepwise)
model.forward = ols_step_forward_p(q1.lm, pent = 0.01, details = FALSE)
model.forward
model.lm.forward = lm(Weight ~ MomWtGain + MomSmoke + Black, data = df)
summary(model.lm.forward)
model.backward = ols_step_backward_p(q1.lm, prem = 0.01, details = FALSE)
model.backward
model.lm.backward = lm(Weight ~ MomWtGain + MomSmoke + Black, data = df)
summary(model.lm.backward)
adj_r_sq = ols_step_best_subset(q1.lm)
adj_r_sq
model.lm.adj_r_sq = lm(Weight ~ Black + Married + Boy + MomSmoke + Ed + MomWtGain,
data = df)
summary(model.lm.adj_r_sq)
q2.lm.stepwise = lm(Weight ~ MomWtGain + MomSmoke + Black, data = df)
summary(q2.lm.stepwise)
par(mfrow=c(2,2))
plot(q2.lm.stepwise, which = 1:4)
q2.cooks = which(cooks.distance(q2.lm.stepwise) > 0.115)
df[q2.cooks, ]
dim(df[-q2.cooks, ])
q2.refitted.step = lm(Weight ~ MomWtGain + MomSmoke + Black, data = df[-q2.cooks, ])
summary(q2.refitted.step)
model.null = glm(Weight_Gr ~ 1,
data = df,
family = "binomial")
model.full = glm(Weight_Gr ~ Black + Married + Boy + MomSmoke + Ed + MomAge + MomWtGain
+ Visit,
data = df,
family = "binomial")
step.models.AIC = step(model.null,
scope = list(upper = model.full),
direction = "both",
test = "Chisq",
trace = F)
summary(step.models.AIC)
step.models.BIC = step(model.null,
scope = list(upper = model.full),
direction = "both",
test = "Chisq",
trace = F,
k = log(nrow(df)))
summary(step.models.BIC)
glm.model.bic = glm(Weight_Gr ~ MomWtGain + MomSmoke + MomAge,
data = df,
family = "binomial")
summary(glm.model.bic)
inf.id = which(cooks.distance(glm.model.bic) > 0.1)
inf.id
dim(df[inf.id])
odds_ratio = exp(glm.model.bic$coefficients)
round(odds_ratio, 3)
sample.prop = mean(df$Weight_Gr)
sample.prop
fit.prob = predict(glm.model.bic, type = "response")
pred.class = ifelse(fit.prob > sample.prop, 1, 0)
mean(df$Weight_Gr != pred.class)
hoslem.test(glm.model.bic$y, fitted(glm.model.bic), g = 10)
View(step.models.BIC)
View(glm.model.bic)
View(df)
library(DescTools); library(ResourceSelection)
setwd("/Users/c2cypher/codebase/msda/msda-grad-school/sta-6443-902-data_analytics_algorithms/datasets")
resp = read.csv("resp.csv", header=TRUE)
str(resp)
resp$Cent = as.factor(resp$Cent)
resp$BL = as.factor(resp$BL)
View(q2.refitted.step)
View(resp)
library(DescTools); library(ResourceSelection)
setwd("/Users/c2cypher/codebase/msda/msda-grad-school/sta-6443-902-data_analytics_algorithms/datasets")
resp = read.csv("resp.csv", header=TRUE)
str(resp)
resp$Cent = as.factor(resp$Cent)
resp$BL = as.factor(resp$BL)
model.full = glm(V1 ~ .-V4, data=resp, family = binomial) # full model: all predictors
glm.resp <- step(model.full,
direction="backward",test="Chisq", trace = F, k=log(nrow(resp)))
summary(glm.resp)
exp(glm.resp$coefficients)
###########################################
## goodness-of-fit check:  Hosmer Lemeshow test
hoslem.test(glm.resp$y, fitted(glm.resp), g=10)  # function in package "ResourceSelection"
#############################################
## residual plots
resid.d<-residuals(glm.resp, type = "deviance")
resid.p<-residuals(glm.resp, type = "pearson")
std.res.d<-residuals(glm.resp, type = "deviance")/sqrt(1 - hatvalues(glm.resp)) # standardized deviance residuals
std.res.p <-residuals(glm.resp, type = "pearson")/sqrt(1 - hatvalues(glm.resp)) # standardized pearson residuals
dev.new(width = 1000, height = 1000, unit = "px")
par(mfrow=c(1,2))
plot(std.res.d[glm.resp$model$V1==0], col = "red",
ylim = c(-3.5,3.5), ylab = "std. deviance residuals", xlab = "ID")
points(std.res.d[glm.resp$model$V1==1], col = "blue")
plot(std.res.p[glm.resp$model$V1==0], col = "red",
ylim = c(-3.5,3.5), ylab = "std. Pearson residuals", xlab = "ID")
points(std.res.p[glm.resp$model$V1==1], col = "blue")
dev.new(width = 1000, height = 1000, unit = "px")
plot(glm.resp, which = 4, id.n = 5)  # visual inspection
(inf.id=which(cooks.distance(glm.resp)>0.05))
fit.prob <- predict(glm.resp, type = "response") # estimated (fitted) probabilities
pred.class.1 <- ifelse(fit.prob > 0.5, 1, 0) # classification with 0.5 threshold
head(pred.class.1, 10)
cbind(resp[c("Treat", "BL", "V1")], fit.prob, pred.class.1)
(sample.prop=mean(resp$V1)) # sample proportion
pred.class.2 <- ifelse(fit.prob > sample.prop, 1, 0) # classification with sample proportion threshold
cbind(resp[c("Treat", "BL", "V1")], fit.prob, pred.class.2)
## 3. calculate misclassification rate for each threshold
mean(resp$V1 != pred.class.1)  # misclassification rate from 0.5 threshold
mean(resp$V1 != pred.class.2)  # misclassification rate from sample proportion threshold
library(DescTools);
library(ResourceSelection)
setwd("/Users/c2cypher/codebase/msda/msda-grad-school/sta-6443-902-data_analytics_algorithms/datasets")  # need to change this path
#################################################
# Example 1: plasma data
#################################################
plasma=read.csv("plasma.csv",header=TRUE)
str(plasma)
##############################################
## logistic regression
glm.plasma0 = glm(esr ~ fibrinogen+gamma, data = plasma, family = "binomial")
summary(glm.plasma0)
## odds ratios
OR=exp(glm.plasma0$coefficients)
round(OR, 3)
## Remove insignificant terms, refit and interpret
glm.plasma = glm(esr ~ fibrinogen, data = plasma, family = "binomial")
summary(glm.plasma)
OR2=exp(glm.plasma$coefficients)
round(OR2, 3)
## Hosmer Lemeshow test
hoslem.test(glm.plasma$y, fitted(glm.plasma), g=10)  # function in package "ResourceSelection"
## psuedo R^2
pseudo.r2<-PseudoR2(glm.plasma, which =  c("McFadden", "Nagel", "CoxSnell"))
round(pseudo.r2, 3)
############################################
## diagnostics
###########################################
## 1. Residual plots
resid.d<-residuals(glm.plasma, type = "deviance")
resid.p<-residuals(glm.plasma, type = "pearson")
std.res.d<-residuals(glm.plasma, type = "deviance")/sqrt(1 - hatvalues(glm.plasma)) # standardized deviance residuals
std.res.p <-residuals(glm.plasma, type = "pearson")/sqrt(1 - hatvalues(glm.plasma)) # standardized pearson residuals
dev.new(width = 1000, height = 1000, unit = "px")
par(mfrow=c(1,2))
plot(std.res.d[glm.plasma$model$esr==0], col = "red",
ylim = c(-3.5,3.5), ylab = "std. deviance residuals", xlab = "ID")
points(std.res.d[glm.plasma$model$esr==1], col = "blue")
plot(std.res.p[glm.plasma$model$esr==0], col = "red",
ylim = c(-3.5,3.5), ylab = "std. Pearson residuals", xlab = "ID")
points(std.res.p[glm.plasma$model$esr==1], col = "blue")
dev.new(width = 1000, height = 1000, unit = "px")
plot(glm.plasma, which = 4, id.n = 5)  # visual inspection
# which observation has cook's d larger than 0.4?
(inf.id=which(cooks.distance(glm.plasma)>0.4))
amputation=read.csv("amputation.csv",header = TRUE)
#################################################
# Example 1: plasma data
#################################################
plasma=read.csv("plasma.csv",header=TRUE)
str(plasma)
##############################################
## logistic regression
glm.plasma0 = glm(esr ~ fibrinogen+gamma, data = plasma, family = "binomial")
summary(glm.plasma0)
View(plasma)
View(plasma)
fit.prob <- predict(glm.resp, type = "response") # estimated (fitted) probabilities
pred.class.1 <- ifelse(fit.prob > 0.5, 1, 0) # classification with 0.5 threshold
head(pred.class.1, 10)
cbind(resp[c("Treat", "BL", "V1")], fit.prob, pred.class.1)
(sample.prop=mean(resp$V1)) # sample proportion
pred.class.2 <- ifelse(fit.prob > sample.prop, 1, 0) # classification with sample proportion threshold
cbind(resp[c("Treat", "BL", "V1")], fit.prob, pred.class.2)
head(pred.class.1)
fit.prob <- predict(glm.resp, type = "response") # estimated (fitted) probabilities
pred.class.1 <- ifelse(fit.prob > 0.5, 1, 0) # classification with 0.5 threshold
head(pred.class.1)
pred.class.1
count(pred.class.1 == 1)
resp
cbind(resp[c("Treat", "BL", "V1")], fit.prob, pred.class.2)
?cbind
resp
(sample.prop=mean(resp$V1)) # sample proportion
pred.class.2 <- ifelse(fit.prob > sample.prop, 1, 0) # classification with sample proportion threshold
cbind(resp[c("Treat", "BL", "V1")], fit.prob, pred.class.2)
## 3. calculate misclassification rate for each threshold
mean(resp$V1 != pred.class.1)  # misclassification rate from 0.5 threshold
mean(resp$V1 != pred.class.2)  # misclassification rate from sample proportion threshold
fit.prob <- predict(glm.resp, type = "response") # estimated (fitted) probabilities
pred.class.1 <- ifelse(fit.prob > 0.5, 1, 0) # classification with 0.5 threshold
head(pred.class.1, 10)
cbind(resp[c("Treat", "BL", "V1")], fit.prob, pred.class.1)
resp
pred.class.1 <- ifelse(fit.prob > 0.5, 1, 0) # classification with 0.5 threshold
head(pred.class.1, 10)
cbind(resp[c("Treat", "BL", "V1")], fit.prob, pred.class.1)
resp
sample.prop=mean(resp$V1) # sample proportion
pred.class.2 <- ifelse(fit.prob > sample.prop, 1, 0) # classification with sample proportion threshold
cbind(resp[c("Treat", "BL", "V1")], fit.prob, pred.class.2)
## 3. calculate misclassification rate for each threshold
mean(resp$V1 != pred.class.1)  # misclassification rate from 0.5 threshold
mean(resp$V1 != pred.class.2)  # misclassification rate from sample proportion threshold
fit.prob <- predict(glm.resp, type = "response") # estimated (fitted) probabilities
pred.class.1 <- ifelse(fit.prob > 0.5, 1, 0) # classification with 0.5 threshold
head(pred.class.1, 10)
combined.dat = cbind(resp[c("Treat", "BL", "V1")], fit.prob, pred.class.1)
combined.dat
fit.prob <- predict(glm.resp, type = "response") # estimated (fitted) probabilities
pred.class.1 <- ifelse(fit.prob > 0.5, 1, 0) # classification with 0.5 threshold
head(pred.class.1, 10)
combined.dat = cbind(resp[c("Treat", "BL", "V1")], fit.prob, pred.class.1)
combined.dat
sample.prop=mean(resp$V1) # sample proportion
pred.class.2 <- ifelse(fit.prob > sample.prop, 1, 0) # classification with sample proportion threshold
cbind(resp[c("Treat", "BL", "V1")], fit.prob, pred.class.2)
cbind(fit.prob, pred.class.1, pred.class.2)
## 3. calculate misclassification rate for each threshold
mean(resp$V1 != pred.class.1)  # misclassification rate from 0.5 threshold
mean(resp$V1 != pred.class.2)  # misclassification rate from sample proportion threshold
fit.prob <- predict(glm.resp, type = "response") # estimated (fitted) probabilities
pred.class.1 <- ifelse(fit.prob > 0.5, 1, 0) # classification with 0.5 threshold
head(pred.class.1, 10)
combined.dat = cbind(resp[c("Treat", "BL", "V1")], fit.prob, pred.class.1)
combined.dat
sample.prop = mean(resp$V1) # sample proportion
pred.class.2 <- ifelse(fit.prob > sample.prop, 1, 0) # classification with sample proportion threshold
cbind(fit.prob, pred.class.1, pred.class.2)
## 3. calculate misclassification rate for each threshold
mean(resp$V1 != pred.class.1)  # misclassification rate from 0.5 threshold
mean(resp$V1 != pred.class.2)  # misclassification rate from sample proportion threshold
View(resp)
sample.prop = mean(resp$V1) # sample proportion
sample.prop
library(tidyverse)
library(olsrr)
library(car)
library(ResourceSelection)
library(DescTools)
setwd("/Users/c2cypher/codebase/msda/msda-grad-school/sta-6443-902-data_analytics_algorithms/exam_2/final_exam")
df = read.csv('birthweight_final.csv')
df$Black = as.factor(df$Black)
df$Married = as.factor(df$Married)
df$Boy = as.factor(df$Boy)
df$MomSmoke = as.factor(df$MomSmoke)
df$Ed = as.factor(df$Ed)
str(df)
# Exercise 1)
q1.lm = lm(Weight ~ Black + Married + Boy + MomSmoke + Ed + MomAge + MomWtGain +
Visit, data = df)
summary(q1.lm)
model.stepwise = ols_step_both_p(q1.lm, pent = 0.01, prem = 0.01, details = FALSE)
model.stepwise
model.lm.stepwise = lm(Weight ~ MomWtGain + MomSmoke + Black, data = df)
summary(model.lm.stepwise)
model.forward = ols_step_forward_p(q1.lm, pent = 0.01, details = FALSE)
model.forward
model.lm.forward = lm(Weight ~ MomWtGain + MomSmoke + Black, data = df)
summary(model.lm.forward)
model.backward = ols_step_backward_p(q1.lm, prem = 0.01, details = FALSE)
model.backward
model.lm.backward = lm(Weight ~ MomWtGain + MomSmoke + Black, data = df)
summary(model.lm.backward)
adj_r_sq = ols_step_best_subset(q1.lm)
adj_r_sq
model.lm.adj_r_sq = lm(Weight ~ Black + Married + Boy + MomSmoke + Ed + MomWtGain,
data = df)
summary(model.lm.adj_r_sq)
q2.lm.stepwise = lm(Weight ~ MomWtGain + MomSmoke + Black, data = df)
summary(q2.lm.stepwise)
par(mfrow=c(2,2))
plot(q2.lm.stepwise, which = 1:4)
q2.cooks = which(cooks.distance(q2.lm.stepwise) > 0.115)
df[q2.cooks, ]
dim(df[-q2.cooks, ])
q2.refitted.step = lm(Weight ~ MomWtGain + MomSmoke + Black, data = df[-q2.cooks, ])
summary(q2.refitted.step)
# Exercise 2)
model.null = glm(Weight_Gr ~ 1,
data = df,
family = "binomial")
model.full = glm(Weight_Gr ~ Black + Married + Boy + MomSmoke + Ed + MomAge + MomWtGain
+ Visit,
data = df,
family = "binomial")
step.models.AIC = step(model.null,
scope = list(upper = model.full),
direction = "both",
test = "Chisq",
trace = F)
summary(step.models.AIC)
step.models.BIC = step(model.null,
scope = list(upper = model.full),
direction = "both",
test = "Chisq",
trace = F,
k = log(nrow(df)))
summary(step.models.BIC)
glm.model.bic = glm(Weight_Gr ~ MomWtGain + MomSmoke + MomAge,
data = df,
family = "binomial")
summary(glm.model.bic)
inf.id = which(cooks.distance(glm.model.bic) > 0.1)
inf.id
dim(df[inf.id])
odds_ratio = exp(glm.model.bic$coefficients)
round(odds_ratio, 3)
sample.prop = mean(df$Weight_Gr)
sample.prop
fit.prob = predict(glm.model.bic, type = "response")
pred.class = ifelse(fit.prob > sample.prop, 1, 0)
mean(df$Weight_Gr != pred.class)
hoslem.test(glm.model.bic$y, fitted(glm.model.bic), g = 10)
View(q1.lm)
